{
 "cells": [
  {
   "cell_type": "raw",
   "id": "57e6d59e-516d-4da6-86e6-25e3be241b05",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Demystifying PyTorch Static Quantization\"\n",
    "description: \"A deep dive into how PyTorch performs inference with quantized models.\"\n",
    "author: \n",
    " - name: \"Haruka Doyu\"\n",
    "   email: haruka.doyu@ninjalabo.ai\n",
    "date: \"08/12/2024\"\n",
    "draft: false\n",
    "categories:\n",
    "  - Tech\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796c46f-542e-4ba9-b0f0-596e4e9b41c6",
   "metadata": {},
   "source": [
    "In the world of machine learning, optimizing model performance and efficiency is crucial, especially for deploying models on edge devices with limited resources. One powerful technique to achieve this is quantization, which reduces the precision of the numbers used in a modelâ€™s computations. PyTorch supports two types of quantization: dynamic and static. Dynamic quantization adjusts the precision of weights at runtime, while static quantization involves converting the model's weights and activations to lower precision based on calibration data. This article will focus on statically quantized models, breaking down the core concepts and steps involved in PyTorch's approach to inference with these models.\n",
    "\n",
    "Note: This article assumes you are already familiar with quantization, particularly static quantization. If not, I recommend checking out the some materials, e.g., our technology page for an introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4f6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "import torch\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "from torch.ao.quantization.quantize_fx import convert_fx, prepare_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a0dd8a-5779-4bbe-9c7e-7f84ae777887",
   "metadata": {},
   "source": [
    "Let's start by creating a Quantizer class to quantize a PyTorch model. For an introduction to PyTorch quantization, you can refer to the official documentation. As an example, I will use the Imagenette2-320 dataset and the ResNet18 model. For convenience, I will leverage the Fastai learner to streamline this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2124e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer():\n",
    "    def __init__(self, backend=\"x86\"):\n",
    "        self.qconfig = get_default_qconfig_mapping(backend)\n",
    "        torch.backends.quantized.engine = backend\n",
    "\n",
    "    def quantize(self, model, calibration_dls):\n",
    "        x, _ = calibration_dls.valid.one_batch()\n",
    "        model_prepared = prepare_fx(model.eval(), self.qconfig, x)\n",
    "        with torch.no_grad():\n",
    "            _ = [model_prepared(xb.to('cpu')) for xb, _ in calibration_dls.valid]\n",
    "\n",
    "        return model_prepared, convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae27a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE_320, data=Path.cwd()/'data')\n",
    "dls = ImageDataLoaders.from_folder(path, valid='val', item_tfms=Resize(224),\n",
    "                                   batch_tfms=Normalize.from_stats(*imagenet_stats))\n",
    "learn = vision_learner(dls, resnet18)\n",
    "model_prepared, qmodel = Quantizer(\"qnnpack\").quantize(learn.model, learn.dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af27d22-56f3-4d2a-b14c-a917c1dd6bfc",
   "metadata": {},
   "source": [
    "In static quantization, the scaling factors and zero points for weights and activations are determined after model calibration but before inference. In this context, we are using per-tensor quantization, which means that there is a single scaling factor and zero point applied uniformly across all elements in each tensor of a layer. This approach is straightforward and computationally efficient, as it simplifies the quantization process by treating the entire tensor as a whole.\n",
    "\n",
    "In the above cell, `model_prepared` instance represents the model after it has recorded the range of activations across a validation dataset. This model contains the necessary information about the model structure and activation ranges, from which the scaling factors and zero points are calculated. Below is an example of the quantization parameters for some activations. The `HistogramObserver` is used to record the activation ranges. The first output shows the quantized parameters of the first activation, which is the model input, while the second output shows the quantization parameters of the second activation, which is the output of the first `Conv2d` + `ReLU` layer. In PyTorch, to avoid redundant quantization and dequantization processes between layers, batch normalization is folded into the preceding layer (batch normalization folding), and the ReLU layer is fused with the layer it follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b55b5f-3305-4b00-aed8-eb44fa678a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistogramObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
      "Scaling Factor: 0.018649335950613022\n",
      "Zero Point: 114\n",
      "\n",
      "HistogramObserver(min_val=0.0, max_val=7.000605583190918)\n",
      "Scaling Factor: 0.011327190324664116\n",
      "Zero Point: 0\n",
      "\n",
      "HistogramObserver(min_val=0.0, max_val=7.000605583190918)\n",
      "Scaling Factor: 0.011327190324664116\n",
      "Zero Point: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example activation quantization parameters\n",
    "for i in range(3):\n",
    "    attr = getattr(model_prepared, f\"activation_post_process_{i}\")\n",
    "    scale, zero_p = attr.calculate_qparams()\n",
    "    print(\"{}\\nScaling Factor: {}\\nZero Point: {}\\n\".format(attr, scale.item(), zero_p.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90894e0b-4877-44d2-abf0-12295b6249d9",
   "metadata": {},
   "source": [
    "`qmodel` instance represents the quantized model. It contains quantized weights, along with their associated scaling factor and zero point, as well as the scaling factor and zero point for activations. Additionally, it includes some non-quantized parameters, which I will explain later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db0a722-9b10-473c-93c9-3694e1247cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (0): Module(\n",
       "    (0): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Module(\n",
       "      (0): Module(\n",
       "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008901300840079784, zero_point=0, padding=(1, 1))\n",
       "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.024013830348849297, zero_point=149, padding=(1, 1))\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.007031331304460764, zero_point=0, padding=(1, 1))\n",
       "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.031252723187208176, zero_point=156, padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): Module(\n",
       "      (0): Module(\n",
       "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.007301042787730694, zero_point=0, padding=(1, 1))\n",
       "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.019116230309009552, zero_point=124, padding=(1, 1))\n",
       "        (downsample): Module(\n",
       "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.01664934679865837, zero_point=135)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.008282394148409367, zero_point=0, padding=(1, 1))\n",
       "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.02566305175423622, zero_point=137, padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): Module(\n",
       "      (0): Module(\n",
       "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.010484358295798302, zero_point=0, padding=(1, 1))\n",
       "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.02675902470946312, zero_point=90, padding=(1, 1))\n",
       "        (downsample): Module(\n",
       "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.008271278813481331, zero_point=162)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00832998938858509, zero_point=0, padding=(1, 1))\n",
       "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.027811763808131218, zero_point=142, padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): Module(\n",
       "      (0): Module(\n",
       "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.006999513134360313, zero_point=0, padding=(1, 1))\n",
       "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.023119885474443436, zero_point=140, padding=(1, 1))\n",
       "        (downsample): Module(\n",
       "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.02033478580415249, zero_point=128)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.006345659960061312, zero_point=0, padding=(1, 1))\n",
       "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.12105856835842133, zero_point=88, padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Module(\n",
       "    (0): Module(\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): QuantizedDropout(p=0.25, inplace=False)\n",
       "    (4): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.08005672693252563, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): QuantizedDropout(p=0.5, inplace=False)\n",
       "    (8): QuantizedLinear(in_features=512, out_features=10, scale=0.10456003248691559, zero_point=150, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96778a-bd95-4924-b03c-c69e56d532c9",
   "metadata": {},
   "source": [
    "Let's investigate the first layer of `qmodel`, i.e., quantized Conv2d + ReLU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8729a79c-1b9a-49df-a00f-6a7db882243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\n",
      "Weight Scale: 0.0030892190989106894, Weight Zero Point: 0\n",
      "Output Scaling Factor: 0.011327190324664116, Output Zero Point: 0\n",
      "\n",
      "Example weights: tensor([-0.0031,  0.0000,  0.0000,  0.0185,  0.0124,  0.0031, -0.0031],\n",
      "       size=(7,), dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.0030892190989106894, zero_point=0)\n",
      "In integer representation: tensor([-1,  0,  0,  6,  4,  1, -1], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "layer = qmodel._modules['0']._modules['0']\n",
    "print(layer)\n",
    "print(\"Weight Scale: {}, Weight Zero Point: {}\".format(layer.weight().q_scale(),\n",
    "                                                       layer.weight().q_zero_point()))\n",
    "print(\"Output Scaling Factor: {}, Output Zero Point: {}\\n\".format(layer.scale, \n",
    "                                                                  layer.zero_point))\n",
    "\n",
    "print(\"Example weights:\", layer.weight()[0, 0, 0])\n",
    "print(\"In integer representation:\", layer.weight()[0, 0, 0].int_repr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9a90d-417f-4c7c-9d1c-be244fe25fe6",
   "metadata": {},
   "source": [
    "As shown above, the quantized layer contains two scaling factors and zero points: one for the weights and another for the output activation. You may have noticed that the output scaling factor and zero point are the same as those displayed in the second cell above, as they represent the same activation.\n",
    "\n",
    "What about biases, which I haven't discussed yet? In PyTorch, biases are not quantized during the initial quantization stage. Instead, they are quantized at inference. Although bias quantization could technically be performed at the same stage as weight quantization, PyTorch does not display the quantized biases at this point. The formula for bias quantization in PyTorch is\n",
    "$$\n",
    "b_q = round(b / (si * sw))\n",
    "$$\n",
    ", where $b_q$ is quantized bias, $b$ is bias before quantization, $si$ is input activation scale and $sw$ is weight scale.\n",
    "For more details, you can refer to this [discussion](https://discuss.pytorch.org/t/is-bias-quantized-while-doing-pytorch-static-quantization/146416/6).\n",
    "\n",
    "In addition, the model may include other non-quantized parameters, such as parameters in batch normalization layers that are not fused. This is likely because quantizing the activations in these layers would not provide significant benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4573d-4889-48c6-b302-4005d260caaf",
   "metadata": {},
   "source": [
    "## What happens during inference?\n",
    "\n",
    "This section demonstrates how calculations are performed in the quantized model during inference. To illustrate this, I calculate the output of the first convolutional layer and validate it against the actual result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18e6d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: tensor([163, 119, 155, 138, 126, 164, 115, 132, 115, 166], dtype=torch.uint8)\n",
      "Example output: tensor([10,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "layer_input = None\n",
    "layer_output = None\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global layer_output, layer_input\n",
    "    layer_input = input\n",
    "    layer_output = output\n",
    "\n",
    "img = torch.rand([1, 3, 224, 224])\n",
    "hook = qmodel._modules['0']._modules['0'].register_forward_hook(hook_fn)\n",
    "output = qmodel(img)\n",
    "hook.remove()\n",
    "print(\"Example input:\", layer_input[0][0,0,0,:10].int_repr())\n",
    "print(\"Example output:\", layer_output[0,0,0,:10].int_repr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0c8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quantize(x, qparams, itype):\n",
    "    xtype = torch.iinfo(itype)\n",
    "    return torch.clamp(torch.round(x / qparams[0]) + qparams[1], min=xtype.min, max=xtype.max)\n",
    "\n",
    "def dequantize(x, qparams):\n",
    "    return (x - qparams[1]) * qparams[0]\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "    return torch.tensor(col)\n",
    "\n",
    "# first use im2col, which is efficient way to perform Conv2d operation\n",
    "inp = im2col(img, 7, 7, 2, 3).float()\n",
    "# quantize input values using input scale and zero point\n",
    "inp = quantize(inp, [layer_input[0].q_scale(), layer_input[0].q_zero_point()], torch.uint8)\n",
    "# get quantized weights, weight scale and quantize biases\n",
    "w = qmodel._modules['0']._modules['0'].weight().int_repr().reshape(64, -1).float()\n",
    "sw = qmodel._modules['0']._modules['0'].weight().q_scale()\n",
    "b = quantize(qmodel._modules['0']._modules['0'].bias(),\n",
    "             [layer_input[0].q_scale() * sw, 0], torch.int32)\n",
    "b = b.reshape(1,64,1,1).detach()\n",
    "# calculate matmul in Conv2d and add biases\n",
    "out = (w @ (inp.T - layer_input[0].q_zero_point())).view(1,64,112,112) + b\n",
    "# dequantize, perform ReLU and quantize based on output scale and zero point\n",
    "out = out * sw * layer_input[0].q_scale()\n",
    "out = torch.relu(out)\n",
    "out = quantize(out, [layer_output.q_scale(), layer_output.q_zero_point()], torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cce797-064d-4d18-9b0c-764e58e884a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  tensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "torch.allclose(out, layer_output.int_repr().float())\n",
    "print(\"Output: \", out[0, 0, 0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0b950-0cb4-418c-90a0-ab8b4e1790cb",
   "metadata": {},
   "source": [
    "Our calculation matches the actual result, which is a good sign. Although some operations in PyTorch's implementation might be performed in a different order, the overall process is likely very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa862e-d23e-4a8c-9826-8ed49fd56ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
