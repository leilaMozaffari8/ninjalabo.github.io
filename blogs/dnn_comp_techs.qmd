---
title: "Comprehensive Overview of DNN Model Compression Techniques"
author: 
 - name: "Hiroshi Doyu"
   email: hiroshi.doyu@ninjalabo.ai
date: "08/18/2024"
draft: false
categories:
  - Tech
  - DNN
---
# DNN Model Compression Techniques

Deep Neural Networks (DNNs) have achieved remarkable success across
various domains, from image recognition to natural language
processing. However, their deployment, especially in
resource-constrained environments like mobile devices or edge
computing, is often hindered by their size and computational
demands. Model compression techniques are essential to address these
challenges, enabling the use of DNNs in real-world applications
without sacrificing too much performance. This article provides a
comprehensive overview of the major DNN model compression techniques,
presenting a big picture of how each approach contributes to making
neural networks more efficient.



![](images/dnn_comp_all.png)

## 1. Quantization

**Quantization** reduces the precision of the numbers representing model parameters, thus decreasing the memory and computational requirements. The key methods include:

- **Post-Training Quantization (PTQ)**: This technique converts the weights and activations of a pre-trained model to lower precision (e.g., from 32-bit floating point to 8-bit integers) without requiring further training. Variants include:
  - **Static Quantization**: Quantizes weights and activations based on a representative dataset.
  - **Dynamic Quantization**: Quantizes activations dynamically during inference.
  - **Weight-Only Quantization**: Applies quantization only to the model weights.
  - **Per-Channel Quantization**: Quantizes each channel separately, allowing for more precise control.
  - **Non-Uniform Quantization**: Uses non-uniform quantization scales, optimizing for ranges that are more sensitive to accuracy.

- **Quantization-Aware Training (QAT)**: Unlike PTQ, QAT involves training the model with simulated quantization effects, allowing the network to learn to be robust to the lower precision. This typically results in better performance compared to PTQ.

- **Mixed Precision Quantization**: Combines different precisions (e.g., 16-bit, 8-bit) within the same model, selecting the precision dynamically based on the importance of different layers or operations. This approach balances computational efficiency with model accuracy.

## 2. Pruning

**Pruning** involves removing less important parameters (e.g., weights, neurons, filters) from the network, effectively "trimming" the model without significant loss in performance. Pruning can be categorized into:

- **Unstructured Pruning**: Removes individual weights based on certain criteria, such as magnitude-based pruning, where weights with the smallest magnitude are removed. This type of pruning can lead to sparse matrices that are harder to accelerate on standard hardware.
  - **Lottery Ticket Hypothesis**: Suggests that within a large network, there exist smaller subnetworks ("winning tickets") that can be trained to achieve performance similar to the original network.

- **Structured Pruning**: In contrast to unstructured pruning, this method removes entire structures, such as filters or channels, making the resulting model more hardware-friendly.
  - **Filter Pruning**: Removes entire filters in convolutional layers.
  - **Channel Pruning**: Prunes entire channels across feature maps.
  - **Block Pruning**: Removes blocks of weights or layers.
  - **Automated Gradual Pruning**: Gradually removes parameters during training based on a predefined schedule, as implemented in tools like FasterAI.

- **Dynamic Pruning**: Adjusts the pruning strategy during inference or training based on runtime conditions, ensuring the model adapts to changing computational constraints.

## 3. Knowledge Distillation

**Knowledge Distillation** is a technique where a smaller "student" model is trained to mimic the behavior of a larger "teacher" model. The student model learns not only from the labeled data but also from the soft predictions of the teacher, which provides richer information about the data.

- **Teacher-Student Distillation**: The classic approach where the student model is trained using the outputs of a pre-trained teacher model.
  - **Soft Targets**: The student model is trained to match the teacher's softened outputs (probabilities) rather than the hard labels.
  - **Intermediate Layer Distillation**: The student learns from the intermediate representations of the teacher model, not just the final output.

- **Self-Distillation**: A single model distills knowledge from itself by using its own predictions from previous training iterations as targets.

- **Cross-Model Distillation**: Distillation occurs between different types of models, such as distilling from an ensemble of models to a single student model.

## 4. Sparsification

**Sparsification** aims to increase the sparsity of the network by zeroing out insignificant parameters, similar to pruning but typically performed during training:

- **Granularity Levels**: Sparsification can be applied at various levels, such as individual weights, vectors, kernels, or entire filters.
- **Context**: 
  - **Local Sparsification**: Applied independently to each layer.
  - **Global Sparsification**: Considers the entire network when determining which parameters to sparsify.
- **Criteria**: Various criteria, such as L1 norm or Taylor expansion, are used to decide which parameters to zero out.
- **Schedule**: The process can be one-shot (all at once), iterative (gradual), or based on automated gradual pruning schedules.

## 5. Low-Rank Factorization

**Low-Rank Factorization** reduces the dimensionality of the model parameters by decomposing matrices or tensors into products of lower-dimensional entities:

- **Matrix Factorization**:
  - **Singular Value Decomposition (SVD)**: Decomposes weight matrices into products of smaller matrices, effectively reducing the model size.
  
- **Tensor Factorization**:
  - **CP Decomposition**: Decomposes tensors (multi-dimensional arrays) into sums of outer products of vectors.
  - **Tucker Decomposition**: Generalizes matrix decomposition to higher dimensions by decomposing a tensor into a core tensor multiplied by matrices along each mode.

## 6. Weight Sharing

**Weight Sharing** reduces the storage requirements by sharing weights across different parts of the network:

- **HashNet**: Hashes weights into a smaller set of values to reduce memory usage.
- **Vector Quantization**: Clusters similar weights together and shares the centroid of the clusters.
- **K-Means Clustering**: A popular technique in which weights are clustered, and only the cluster centers are stored.

## 7. Neural Architecture Search (NAS)

**Neural Architecture Search** automates the design of neural networks, optimizing them for specific constraints, such as size, speed, or accuracy:

- **Reinforcement Learning-Based NAS**: Uses reinforcement learning to explore different architectures.
- **Evolutionary Algorithm-Based NAS**: Applies evolutionary strategies to evolve network architectures over successive generations.
- **Gradient-Based NAS**: Utilizes gradients to guide the search for optimal architectures.
- **Hardware-Aware NAS**: Tailors the search to optimize architectures specifically for target hardware, balancing performance with computational efficiency.

## 8. Efficient Architectures

**Efficient Architectures** are specially designed to be lightweight and fast while maintaining high performance. These architectures are often used in mobile and embedded systems:

- **MobileNets (V1, V2, V3)**: Use depthwise separable convolutions to reduce the number of parameters and computations.
- **ShuffleNet**: Introduces channel shuffling to improve efficiency in grouped convolutions.
- **EfficientNet**: Balances network depth, width, and resolution for optimal performance with minimal resources.
- **MCUNet**: Tailored for microcontrollers, focusing on extreme efficiency.
- **SqueezeNet**: Uses squeeze-and-excitation blocks to reduce the number of parameters.
- **TinyEngine**: A neural network engine optimized for TinyML applications on microcontrollers.

## 9. Early Exit Techniques

**Early Exit Techniques** allow a model to make predictions at intermediate layers if the confidence is high, reducing the need for processing through the entire network:

- **BranchyNet**: Adds multiple exits at different depths of the network.
- **Multi-Exit Networks**: Similar to BranchyNet, but with more sophisticated decision mechanisms to determine when to exit early.

## 10. Adaptive Inference

**Adaptive Inference** dynamically adjusts the computational graph of a network during inference based on the input or runtime conditions:

- **Dynamic Computational Graphs**: Modifies the structure of the network on-the-fly during inference.
- **Conditional Computation**: Activates only a subset of the network's layers or neurons based on the input data, reducing the overall computational load.

## 11. Sparse Representations

**Sparse Representations** focus on reducing the number of active parameters in a network, leveraging the inherent redundancy in DNNs:

- **Compressed Sensing**: Recovers a sparse signal from a small number of measurements, applicable in scenarios where data is sparse.
- **Sparse Neural Networks**: Networks explicitly designed with sparsity in mind from the start.
- **Sparse Evolutionary Training**: Evolves networks that start sparse and remain sparse throughout training.

## 12. Regularization Techniques

**Regularization Techniques** encourage the network to develop sparse connections during training by applying penalties to the magnitude of weights:

- **L1/L2 Regularization**: Adds a penalty to the loss function proportional to the sum of absolute or squared values of the weights, encouraging sparsity.

## 13. BatchNorm Folding

**BatchNorm Folding** combines Batch Normalization layers with preceding layers to reduce the computational load during inference:

- **Merging BatchNorm Layers**: BatchNorm layers are merged with the preceding convolutional or fully connected layers during inference, reducing the number of operations.

## 14. Fully-Connected Layer Decomposition

**Fully-Connected Layer Decomposition** reduces the dimensionality of fully connected layers using matrix decomposition techniques:

- **Matrix Decomposition**: Techniques like SVD are applied to decompose large weight matrices into smaller, more efficient representations.


# Resources
Here are the links that you provided and which were used as sources in the discussion:

1. **MIT Efficient AI Course - Fall 2024**  
   [MIT Efficient AI Course - Fall 2024](https://hanlab.mit.edu/courses/2024-fall-65940)

2. **A Visual Guide to Quantization by Maarten Grootendorst**  
   [A Visual Guide to Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)

3. **FasterAI by Nathan Hubens**  
   [FasterAI](https://nathanhubens.github.io/fasterai/)