[
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "Here, we visualize the performance of three runtimes: PyTorch, our custom-built tinyRuntime (both non-quantized and quantized versions), using the ResNet18 model and 100 images from the Imagenette dataset. We focus on four key metrics: accuracy, execution time, model size and memory usage.\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, theme):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),height=600, width=900,\n        template=theme, legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86, \"plotly_dark\")\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on x86\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.551212\n465.410156\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n10.444725\n65.343750\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n71.849900\n35.863281\n11.949406\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, \"plotly_white\")\n\n\n\n\n                                                \n\n\nFigure 2: Performance comparison on ARM\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n7.024174\n469.203125\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n6.610923\n78.375000\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n12.131101\n39.765625\n11.949406",
    "crumbs": [
      "Performance",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "performance.html#x86",
    "href": "performance.html#x86",
    "title": "Quarto Basics",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, theme):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),height=600, width=900,\n        template=theme, legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86, \"plotly_dark\")\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on x86\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.551212\n465.410156\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n10.444725\n65.343750\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n71.849900\n35.863281\n11.949406",
    "crumbs": [
      "Performance",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "performance.html#arm",
    "href": "performance.html#arm",
    "title": "Quarto Basics",
    "section": "",
    "text": "Code\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, \"plotly_white\")\n\n\n\n\n                                                \n\n\nFigure 2: Performance comparison on ARM\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n7.024174\n469.203125\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n6.610923\n78.375000\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n12.131101\n39.765625\n11.949406",
    "crumbs": [
      "Performance",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "TinyML as-a-Service Overview\n\n\n\n\n\nBrief overview of NinjaLABO’s TinyML as-a-Service / AI compression as-a-Service \n\n\n\n\n\nMay 7, 2025\n\n\nHiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nIREE review\n\n\n\n\n\nSome description\n\n\n\n\n\nNov 7, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nImagimob Studio Review\n\n\n\n\n\nBrief overview of Imagimob Studio - A solution for Edge AI applications\n\n\n\n\n\nAug 7, 2024\n\n\nNghi Vo &lt;nghi.vo@ninjalabo.ai&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nAI compression SaaS\n\n\n\n\n\nBrief overview of AI Compression\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML Use Cases\n\n\n\n\n\nUse cases for TinyML offered by NinjaLABO\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "blogs/aicompressionsaas.html",
    "href": "blogs/aicompressionsaas.html",
    "title": "AI compression SaaS",
    "section": "",
    "text": "It has been nearly five years since I first wrote about TinyML and TinyML as-a-Service (TinyMLaaS) on the Ericsson blog.\n\nIn the rapidly evolving landscape of artificial intelligence (AI), efficiency and resource optimization are paramount. AI Compression as-a-Service (ACaaS) and TinyML as-a-Service (TinyMLaaS) emerge as transformative solutions that address the growing demand for deploying AI on resource-constrained devices. These services offer scalable, cost-effective, and high-performance AI capabilities, enabling a new wave of innovation in edge computing.\n\n\nAI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance.\n\n\n\n\nResource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security.\n\n\n\n\nTinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications.\n\n\n\n\n\n\nSmart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring.\n\n\n\n\nAI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "href": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "href": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Resource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "href": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "title": "AI compression SaaS",
    "section": "",
    "text": "TinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "href": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Smart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#conclusion",
    "href": "blogs/aicompressionsaas.html#conclusion",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/iree.html",
    "href": "blogs/iree.html",
    "title": "IREE review",
    "section": "",
    "text": "IREE Review\nImportant review…"
  },
  {
    "objectID": "tinymlaas.html",
    "href": "tinymlaas.html",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "Hiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;, 17th July, 2024"
  },
  {
    "objectID": "tinymlaas.html#what-is-tinyml-ai-compression",
    "href": "tinymlaas.html#what-is-tinyml-ai-compression",
    "title": "TinyML as-a-Service Overview",
    "section": "What is TinyML / AI Compression",
    "text": "What is TinyML / AI Compression\nTinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\nThe Two Phases of Deep Neural Networks (DNNs)\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\nTinyML as-a-Service / AI Compression as-a-Service Workflow\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\nKey Points to Understand\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "NinjaLABO is specialized at compressing AI models (DNN models) along with optimized tinyRuntime via our SaaS platform.",
    "crumbs": [
      "Features",
      "About"
    ]
  },
  {
    "objectID": "blogs/tinymlaas.html",
    "href": "blogs/tinymlaas.html",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "href": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/imagimobstudio.html",
    "href": "blogs/imagimobstudio.html",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is an end-to-end platform designed for developing Edge AI and Machine Learning applications. The platform covers the machine learning workflow from data collection to model deployment in embedded devices to support both experts and non-experts in building production grade models.\n\n\n\n\nImagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C.\n\n\n\n\n\n\nPredictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars.\n\n\n\n\nImagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/imagimobstudio.html#key-features",
    "href": "blogs/imagimobstudio.html#key-features",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C."
  },
  {
    "objectID": "blogs/imagimobstudio.html#use-cases",
    "href": "blogs/imagimobstudio.html#use-cases",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Predictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars."
  },
  {
    "objectID": "blogs/imagimobstudio.html#conclusion",
    "href": "blogs/imagimobstudio.html#conclusion",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/tinymlusecases.html",
    "href": "blogs/tinymlusecases.html",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "In today’s competitive landscape, every company is leveraging AI or exploring its integration into their business operations. As AI models become increasingly sophisticated, the operational expenditures (OPEX) associated with utilizing expensive GPUs in cloud datacenters also rise. Moreover, large AI models often cannot be executed on small devices without relying on cloud GPUs.\nNinjaLABO’s AI model compression (TinyML as-a-Service) addresses these challenges by offering versatile solutions applicable across various industries. Below, we explore specific focus areas and use cases where these solutions are particularly relevant:\n\n\n\nData Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage.\n\n\n\n\n\n\nData Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy.\n\n\n\n\n\n\nData Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security.\n\n\n\n\n\n\nData Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications.\n\n\n\n\n\n\nData Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications.\n\n\n\n\n\n\nData Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "href": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage."
  },
  {
    "objectID": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "href": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy."
  },
  {
    "objectID": "blogs/tinymlusecases.html#industrial-automation",
    "href": "blogs/tinymlusecases.html#industrial-automation",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security."
  },
  {
    "objectID": "blogs/tinymlusecases.html#agriculture",
    "href": "blogs/tinymlusecases.html#agriculture",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#automotive-and-mobility",
    "href": "blogs/tinymlusecases.html#automotive-and-mobility",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#security-and-surveillance",
    "href": "blogs/tinymlusecases.html#security-and-surveillance",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Compress your AI model",
    "section": "",
    "text": "Compress your AI model\nwith tinyMLaaS\nUpload your AI model on device for efficient run to save your costs, time, and cloud space\n\nGet Started See Demo Request Demo\n\n\n\n\n\n\nRun by NinjaLABO.\nNext\nnext"
  }
]