[
  {
    "objectID": "blogs/iree.html",
    "href": "blogs/iree.html",
    "title": "IREE review",
    "section": "",
    "text": "Compiling and optimizing ML models for multiple hardware platforms can be complex and time-consuming. One promising solution on our radar is the Intermediate Representation Execution Environment (IREE), which can significantly simplify this process. Here’s an in-depth look at what I’ve learned about IREE.\n\n\nIREE, or Intermediate Representation Execution Environment, is an end-to-end compiler designed specifically for machine learning (ML) models. It takes models expressed in an intermediate representation using Multi-Level Intermediate Representation (MLIR) and performs hardware-agnostic optimizations and transformations. Unlike traditional compilers, IREE doesn’t directly generate low-level machine code but uses various optimizers and accelerators to do this, making it a versatile framework that supports a variety of hardware platforms.\n\n\n\n\nImport Your Model: Begin by importing your machine learning model into IREE. This can be a model developed using one of the supported frameworks such as TensorFlow, PyTorch, JAX, ONNX, or TensorFlow Lite.\nConfigure Deployment Settings: Specify your deployment configuration, including the target platform (CPU, GPU, etc.), accelerators, and any other constraints relevant to your deployment environment.\nCompile Your Model: Utilize IREE to compile your model. During compilation, IREE optimizes the model’s code for the specified deployment configuration using its end-to-end compiler capabilities.\nRun Your Compiled Model: Once compiled, use IREE’s runtime components to execute your optimized model on the target hardware.\n\n\n\n\n\nIntermediate Representation (IR) and MLIR: IREE uses MLIR (Multi-Level Intermediate Representation) as its IR, which acts like a programming language for expressing machine learning models. This allows for sophisticated optimizations and transformations that are independent of the underlying hardware. IREE supports conversion from a variety of popular ML frameworks including JAX, ONNX, TensorFlow, TensorFlow Lite, and PyTorch into MLIR.\nAutomatic Optimization: Unlike traditional compilers, IREE integrates scheduling and execution logic during compilation, rather than deferring it to runtime. This approach reduces scheduling overhead and enhances efficiency by compiling optimized code tailored to the target hardware at compile time. Additionally, IREE excels in automatic code optimization through compilers it uses, for example parallelization and vectorization are performed automatically (in the case of CPU). This means it can automatically transform the input model code to leverage hardware capabilities effectively, ensuring efficient execution on different platforms.\nWide Hardware Support: IREE supports a broad range of hardware configurations, including various CPUs and GPUs. For example, it offers support for bare-metal, enabling deployment on edge devices with minimal footprint. With the help of the Hardware Abstraction Layer (HAL), IREE can compile for various hardware platforms on any supported machine. This versatility allows developers to optimize and deploy ML models across different hardware environments without the need for extensive platform-specific modifications.\nBindings IREE also provides bindings, which are interfaces that allow access to the IREE compiler and its components from different programming languages, such as Python. For example, you can compile and run models using IREE in the Python interface. This flexibility is crucial for integrating IREE into diverse workflows and leveraging its capabilities from various development environments.\n\n\n\n\nHere was an overview of IREE, emphasizing its capabilities in optimizing models across diverse hardware platforms. For practical examples, please visit IREE’s official documentation at https://iree.dev, where you can find comprehensive guides and examples illustrating how to compile ML models for various hardware configurations."
  },
  {
    "objectID": "blogs/iree.html#what-is-iree",
    "href": "blogs/iree.html#what-is-iree",
    "title": "IREE review",
    "section": "",
    "text": "IREE, or Intermediate Representation Execution Environment, is an end-to-end compiler designed specifically for machine learning (ML) models. It takes models expressed in an intermediate representation using Multi-Level Intermediate Representation (MLIR) and performs hardware-agnostic optimizations and transformations. Unlike traditional compilers, IREE doesn’t directly generate low-level machine code but uses various optimizers and accelerators to do this, making it a versatile framework that supports a variety of hardware platforms."
  },
  {
    "objectID": "blogs/iree.html#workflow-with-iree",
    "href": "blogs/iree.html#workflow-with-iree",
    "title": "IREE review",
    "section": "",
    "text": "Import Your Model: Begin by importing your machine learning model into IREE. This can be a model developed using one of the supported frameworks such as TensorFlow, PyTorch, JAX, ONNX, or TensorFlow Lite.\nConfigure Deployment Settings: Specify your deployment configuration, including the target platform (CPU, GPU, etc.), accelerators, and any other constraints relevant to your deployment environment.\nCompile Your Model: Utilize IREE to compile your model. During compilation, IREE optimizes the model’s code for the specified deployment configuration using its end-to-end compiler capabilities.\nRun Your Compiled Model: Once compiled, use IREE’s runtime components to execute your optimized model on the target hardware."
  },
  {
    "objectID": "blogs/iree.html#key-features-of-iree",
    "href": "blogs/iree.html#key-features-of-iree",
    "title": "IREE review",
    "section": "",
    "text": "Intermediate Representation (IR) and MLIR: IREE uses MLIR (Multi-Level Intermediate Representation) as its IR, which acts like a programming language for expressing machine learning models. This allows for sophisticated optimizations and transformations that are independent of the underlying hardware. IREE supports conversion from a variety of popular ML frameworks including JAX, ONNX, TensorFlow, TensorFlow Lite, and PyTorch into MLIR.\nAutomatic Optimization: Unlike traditional compilers, IREE integrates scheduling and execution logic during compilation, rather than deferring it to runtime. This approach reduces scheduling overhead and enhances efficiency by compiling optimized code tailored to the target hardware at compile time. Additionally, IREE excels in automatic code optimization through compilers it uses, for example parallelization and vectorization are performed automatically (in the case of CPU). This means it can automatically transform the input model code to leverage hardware capabilities effectively, ensuring efficient execution on different platforms.\nWide Hardware Support: IREE supports a broad range of hardware configurations, including various CPUs and GPUs. For example, it offers support for bare-metal, enabling deployment on edge devices with minimal footprint. With the help of the Hardware Abstraction Layer (HAL), IREE can compile for various hardware platforms on any supported machine. This versatility allows developers to optimize and deploy ML models across different hardware environments without the need for extensive platform-specific modifications.\nBindings IREE also provides bindings, which are interfaces that allow access to the IREE compiler and its components from different programming languages, such as Python. For example, you can compile and run models using IREE in the Python interface. This flexibility is crucial for integrating IREE into diverse workflows and leveraging its capabilities from various development environments."
  },
  {
    "objectID": "blogs/iree.html#conclusion",
    "href": "blogs/iree.html#conclusion",
    "title": "IREE review",
    "section": "",
    "text": "Here was an overview of IREE, emphasizing its capabilities in optimizing models across diverse hardware platforms. For practical examples, please visit IREE’s official documentation at https://iree.dev, where you can find comprehensive guides and examples illustrating how to compile ML models for various hardware configurations."
  },
  {
    "objectID": "blogs/ericsson_blog1.html",
    "href": "blogs/ericsson_blog1.html",
    "title": "TinyML as-a-Service and the challenges of machine learning at the edge",
    "section": "",
    "text": "https://www.ericsson.com/en/blog/2019/12/tinyml-as-a-service\n\n\n\nTinyML as-a-Service and the challenges of machine learning at the edge"
  },
  {
    "objectID": "blogs/tinymlusecases.html",
    "href": "blogs/tinymlusecases.html",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "In today’s competitive landscape, every company is leveraging AI or exploring its integration into their business operations. As AI models become increasingly sophisticated, the operational expenditures (OPEX) associated with utilizing expensive GPUs in cloud datacenters also rise. Moreover, large AI models often cannot be executed on small devices without relying on cloud GPUs.\nNinjaLABO’s AI model compression (TinyML as-a-Service) addresses these challenges by offering versatile solutions applicable across various industries. Below, we explore specific focus areas and use cases where these solutions are particularly relevant:\n\n\n\nData Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage.\n\n\n\n\n\n\nData Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy.\n\n\n\n\n\n\nData Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security.\n\n\n\n\n\n\nData Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications.\n\n\n\n\n\n\nData Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications.\n\n\n\n\n\n\nData Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "href": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage."
  },
  {
    "objectID": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "href": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy."
  },
  {
    "objectID": "blogs/tinymlusecases.html#industrial-automation",
    "href": "blogs/tinymlusecases.html#industrial-automation",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security."
  },
  {
    "objectID": "blogs/tinymlusecases.html#agriculture",
    "href": "blogs/tinymlusecases.html#agriculture",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#automotive-and-mobility",
    "href": "blogs/tinymlusecases.html#automotive-and-mobility",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#security-and-surveillance",
    "href": "blogs/tinymlusecases.html#security-and-surveillance",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/imagimobstudio.html",
    "href": "blogs/imagimobstudio.html",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is an end-to-end platform designed for developing Edge AI and Machine Learning applications. The platform covers the machine learning workflow from data collection to model deployment in embedded devices to support both experts and non-experts in building production grade models.\n\n\n\n\nImagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C.\n\n\n\n\n\n\nPredictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars.\n\n\n\n\nImagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/imagimobstudio.html#key-features",
    "href": "blogs/imagimobstudio.html#key-features",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C."
  },
  {
    "objectID": "blogs/imagimobstudio.html#use-cases",
    "href": "blogs/imagimobstudio.html#use-cases",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Predictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars."
  },
  {
    "objectID": "blogs/imagimobstudio.html#conclusion",
    "href": "blogs/imagimobstudio.html#conclusion",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/mvp6.html",
    "href": "blogs/mvp6.html",
    "title": "AI Compression as-a-Service MVP6 review",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review"
  },
  {
    "objectID": "getstarted.html",
    "href": "getstarted.html",
    "title": "Get Started",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review",
    "crumbs": [
      "Get Started"
    ]
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "Here, we visualize the performance of three runtimes: PyTorch, our custom-built tinyRuntime (both non-quantized and quantized versions), using the ResNet18 model and 100 images from the Imagenette dataset. We focus on four key metrics: accuracy, execution time, model size and memory usage.\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, save_image=False):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),height=600, width=900,\n        template=\"plotly_dark\", legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n    \n    if save_image:\n        fig.write_image(\"images/perf_bar.png\")\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86)\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on AMD64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.044205\n469.535156\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n10.442570\n84.660156\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n44.293474\n55.410156\n11.949406\n\n\n\n\n\n\n\n\n\nCode\ndef plot_radar(df, save_image=False):\n    # Normalize the data\n    df = df.copy()\n    df[\"Time\"] = df[\"Time\"].apply(lambda x: x / df[\"Time\"].max())\n    df[\"Accuracy\"] = df[\"Accuracy\"].apply(lambda x: x / 100)\n    df[\"Max memory\"] = df[\"Max memory\"].apply(lambda x: x / df[\"Max memory\"].max())\n    # Split based on runtime and take three relevant columns\n    dfs = [df_split[['Time', 'Max memory', 'Accuracy']] for df_split in split_dataframe(df)]\n    # Create a radar chart\n    categories = ['Time', 'Memory usage', 'Accuracy']\n    runtimes = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n    fig = go.Figure()\n    for i, df in enumerate(dfs):\n        fig.add_trace(go.Scatterpolar(\n            r=df.iloc[-1].values,\n            theta=categories,\n            fill='toself',\n            name=runtimes[i]\n        ))\n\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 1]\n            )\n        ),\n        font=dict(size=16),\n        showlegend=True\n    )\n    fig.add_annotation(\n        text=\"*Values are normalized between 0 and 1\",\n        xref=\"paper\", yref=\"paper\",\n        x=0.5, y=-0.1,\n        showarrow=False,\n        font=dict(size=15)\n    )\n    fig.show()\n    \n    if save_image:\n        fig.write_image(\"images/perf_radar.png\")\n    \nplot_radar(df_x86)\n\n\n\n\n                                                \n\n\nFigure 2: Radar chart of time, memory, and accuracy for AMD64. Quantities are scaled between 0 and 1\n\n\n\n\n\n\n\n\n\nCode\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 3: Performance comparison on ARM64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n6.917829\n454.500000\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n2.775942\n143.234375\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n3.895878\n93.468750\n11.949406\n\n\n\n\n\n\n\n\n\nCode\nplot_radar(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 4: Radar chart of time, memory, and accuracy for ARM64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "performance.html#amd64",
    "href": "performance.html#amd64",
    "title": "Performance",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, save_image=False):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),height=600, width=900,\n        template=\"plotly_dark\", legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n    \n    if save_image:\n        fig.write_image(\"images/perf_bar.png\")\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86)\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on AMD64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.044205\n469.535156\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n10.442570\n84.660156\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n44.293474\n55.410156\n11.949406\n\n\n\n\n\n\n\n\n\nCode\ndef plot_radar(df, save_image=False):\n    # Normalize the data\n    df = df.copy()\n    df[\"Time\"] = df[\"Time\"].apply(lambda x: x / df[\"Time\"].max())\n    df[\"Accuracy\"] = df[\"Accuracy\"].apply(lambda x: x / 100)\n    df[\"Max memory\"] = df[\"Max memory\"].apply(lambda x: x / df[\"Max memory\"].max())\n    # Split based on runtime and take three relevant columns\n    dfs = [df_split[['Time', 'Max memory', 'Accuracy']] for df_split in split_dataframe(df)]\n    # Create a radar chart\n    categories = ['Time', 'Memory usage', 'Accuracy']\n    runtimes = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n    fig = go.Figure()\n    for i, df in enumerate(dfs):\n        fig.add_trace(go.Scatterpolar(\n            r=df.iloc[-1].values,\n            theta=categories,\n            fill='toself',\n            name=runtimes[i]\n        ))\n\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 1]\n            )\n        ),\n        font=dict(size=16),\n        showlegend=True\n    )\n    fig.add_annotation(\n        text=\"*Values are normalized between 0 and 1\",\n        xref=\"paper\", yref=\"paper\",\n        x=0.5, y=-0.1,\n        showarrow=False,\n        font=dict(size=15)\n    )\n    fig.show()\n    \n    if save_image:\n        fig.write_image(\"images/perf_radar.png\")\n    \nplot_radar(df_x86)\n\n\n\n\n                                                \n\n\nFigure 2: Radar chart of time, memory, and accuracy for AMD64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "performance.html#arm64",
    "href": "performance.html#arm64",
    "title": "Performance",
    "section": "",
    "text": "Code\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 3: Performance comparison on ARM64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n6.917829\n454.500000\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n2.775942\n143.234375\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n3.895878\n93.468750\n11.949406\n\n\n\n\n\n\n\n\n\nCode\nplot_radar(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 4: Radar chart of time, memory, and accuracy for ARM64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "TinyML as-a-Service with Helsinki University summer course 2023\n\n\n\n\n\n\nHU\n\n\nTinyMLaaS\n\n\n\nThis is the main repository of Tiny Machine Learning as a Service project for Software Engineering Project course at University of Helsinki, summer 2023.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAbout\n\n\n\n\n\n\nAbout\n\n\nTeam\n\n\n\nThe team of experts in software development, research, and design is working to enable businesses to integrate AI & Machine Learning applications on a wide range of devices without relying on expensive networking & cloud computing services thanks to their solution, TinyML as a Service.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service and the challenges of machine learning at the edge\n\n\n\n\n\n\nEricsson\n\n\n\nThe TinyML as-a-Service project at Ericsson Research sets out to address the challenges that today limit the potential of machine learning (ML) paradigms at the edge of the embedded IoT world. In this post, the second post in our TinyML series, we take a closer look at the technical and non-technical challenges on our journey to making that happen. Learn more below.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we democratize machine learning on IoT devices?\n\n\n\n\n\n\nEricsson\n\n\n\nMaking tiny machine learning widely available on edge IoT devices could prove to be a major leap in smart sensing across industries. Below, we plot the technical milestones to making that happen as part of our ongoing research into TinyML as-a-service.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAI Compression as-a-Service MVP6 review\n\n\n\n\n\n\nNews\n\n\nMVP\n\n\n\nOur internal review of MVP6 usage\n\n\n\n\n\nAug 6, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nEdge Impulse Review\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nCompetitor\n\n\n\nEdge Impulse - A Comprehensive Review of the Leading Edge AI Platform\n\n\n\n\n\nAug 5, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service Overview\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\n\nBrief overview of NinjaLABO’s TinyML as-a-Service / AI compression as-a-Service \n\n\n\n\n\nJul 17, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nIREE review\n\n\n\n\n\n\nTech\n\n\nCompiler\n\n\n\nBrief overview of IREE\n\n\n\n\n\nJul 12, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nImagimob Studio Review\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nCompetitor\n\n\n\nBrief overview of Imagimob Studio - A solution for Edge AI applications\n\n\n\n\n\nJul 8, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nAI compression SaaS\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\n\nBrief overview of AI Compression\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML Use Cases\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nUseCase\n\n\n\nUse cases for TinyML offered by NinjaLABO\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Blog"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html",
    "href": "tutorials/runwalkthrough.html",
    "title": "Run Walkthrough",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html#summary",
    "href": "tutorials/runwalkthrough.html#summary",
    "title": "Run Walkthrough",
    "section": "Summary",
    "text": "Summary\nThe video tutorial is directly obtained from our internal review process of MVP6. It covers the process of creating and reviewing two default runs via project dashboard. Starting with user registration, the current MVP creates a default project and redirects the user to the project dashboard, in which page the main operation takes place. With the project dashboard, the user can specify a model and a dataset to run inference on a selected device. By default, two versions of this inference task are executed (forming two separate runs): a version that uses the original model for inference via Pytorch runtime, and a version that uses a quantized model (8-bit quantization) via tinyruntime (our custom runtime). After both runs are finished, the frontend provides visualizations to compare the results (accuracy), speed, and size of the model for each inference task.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html#features-breakdown",
    "href": "tutorials/runwalkthrough.html#features-breakdown",
    "title": "Run Walkthrough",
    "section": "Features Breakdown",
    "text": "Features Breakdown\n\nRegistration & Project Setup\nThe registration process in MVP6 is simplified, only requiring the user’s email address and a secure password. Email verification is yet to be implemented at this stage. After a successful registration, the user is redirected to the project dashboard with a default name and description, both of which can be modified later on. New projects can be created via sidebar button, which requires a unique name and possibly some description. Different projects can be selected also via the sidebar, which navigates to the project dashboard page of the corresponding selected project.\n\n\nModel & Dataset Selection\nWe are currently using models and datasets uploaded to Hugging Face Hub. The user can choose any publicly accessible models or datasets from Hugging Face. However, the currently recommended ones are our own uploaded models (ninjalabo/resnet18, ninjalabo/resnet34, and ninjalabo/resnet50) and dataset (ninjalabo/imagenette2-320) for consistency in directory structure and accepted file formats.\nThe user can also upload models and datasets to our own UI, which accepts a model file model.pkl and a dataset file in .zip or .tar.gz format. For convenience, the user can download our example model and dataset to easily test this feature without compatability issue.\n\n\nDevice Selection\nThe user can setup and select an edge device in which the inference task(s) shall be executed. The user can register a device with information including its architecture, connectivity details, memory and storage limits, and installation option. Currently, we are supporting devices with arm64 or amd64 architectures, in which the model will be installed in the form of a Docker image in our VM. The model will be optimized to the selected device, and its performance on it is measured and reported (accuracy, execution time, and model size).",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "blogs/edgeimplusestudio.html",
    "href": "blogs/edgeimplusestudio.html",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse has rapidly become a prominent player in the field of machine learning for edge devices. Tailored to cater to a wide range of users, from hobbyists to professional embedded engineers, Edge Impulse Studio offers a comprehensive platform that supports the development, training, and deployment of machine learning models directly onto edge devices.\n\n\nEdge Impulse Studio is a cloud-based platform designed to simplify the modeling and deployment process of machine learning models on edge devices. It’s structured to accommodate a variety of user levels, from beginners exploring ML for the first time to seasoned professionals who require advanced features for enterprise applications. The platform supports multiple subscription tiers: Community, Professional, and Enterprise, each unlocking different levels of functionality and support.\nThe architecture of Edge Impulse is modular and flexible, allowing users to integrate different data sources, customize processing pipelines, and deploy models on various devices. The platform is organized into “impulse blocks” that guide users through the ML workflow, from data acquisition to model deployment. This block-based approach ensures that each step is transparent and manageable, especially for users with lower level of expertise.\n\n\n\nEdge Impulse general architecture & integrations with existing ML tools\n\n\n\n\n\n\n\nEdge Impulse offers a robust suite of tools for data collection and management, making it easy to gather the data necessary for training models.\n\nMultiple Data Collection Methods: Users can collect data directly from devices, upload existing datasets, or even pull data from cloud storage solutions like Amazon S3.\nReal-Time Data Collection: The platform supports real-time data collection through phones, computers, or connected development boards. This versatility allows users to capture images, audio, and motion data using a web-based interface.\nData Explorer: This tool provides powerful data visualization capabilities, helping users to explore and understand their datasets. Enterprise users can also monitor model performance with new data and automate data processing, enhancing the platform’s scalability and efficiency.\nSynthetic Data Generation: While not fully tested, Edge Impulse offers tools to generate synthetic data, which can be valuable in scenarios where real-world data is scarce.\n\n\n\n\nEdge Impulse simplifies the machine learning process with a clear, step-by-step approach:\n\nImpulse Block Design: The platform’s ML workflow is divided into four block types: data extraction, data processing, training, and output. This modular approach ensures that users can easily follow the process and make adjustments as needed.\nDefault and Custom Processing Blocks: Users can choose from default data processing and training blocks or create custom models using tools like Keras. This flexibility is key for users with specific needs or those looking to experiment with novel approaches.\nComprehensive Visualization and Reporting: The platform provides a range of visualization tools, including feature importance analysis, confusion matrices, and performance profiles, which help users evaluate model accuracy and optimize performance.\n\n\n\n\nImpulse Block Design & Workflow\n\n\n\n\n\nOne of the standout features of Edge Impulse is its strong focus on edge devices:\n\nPerformance Profiling: The platform provides performance profiles for selected devices, detailing key metrics like RAM usage, disk space, and processing speed.\nDSP and Transformation Compilation: Users can compile digital signal processing (DSP) transformations and inference code specifically for their target devices, ensuring that models run efficiently on hardware with limited resources.\nHyperparameter Tuning: The platform offers tools for tuning model parameters and selecting the best-performing configurations for a given device, which is crucial for optimizing model deployment on resource-constrained environments.\n\n\n\n\n\nEdge Impulse is a versatile, well-designed platform for developing and deploying machine learning models on edge devices. With its extensive feature set, modular workflow, and strong support for real-time data collection, it caters to both beginners and advanced users alike.\nFor more detailed information, visit the Edge Impulse homepage and Edge Impulse documentation."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#platform-overview",
    "href": "blogs/edgeimplusestudio.html#platform-overview",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse Studio is a cloud-based platform designed to simplify the modeling and deployment process of machine learning models on edge devices. It’s structured to accommodate a variety of user levels, from beginners exploring ML for the first time to seasoned professionals who require advanced features for enterprise applications. The platform supports multiple subscription tiers: Community, Professional, and Enterprise, each unlocking different levels of functionality and support.\nThe architecture of Edge Impulse is modular and flexible, allowing users to integrate different data sources, customize processing pipelines, and deploy models on various devices. The platform is organized into “impulse blocks” that guide users through the ML workflow, from data acquisition to model deployment. This block-based approach ensures that each step is transparent and manageable, especially for users with lower level of expertise.\n\n\n\nEdge Impulse general architecture & integrations with existing ML tools"
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#key-features",
    "href": "blogs/edgeimplusestudio.html#key-features",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse offers a robust suite of tools for data collection and management, making it easy to gather the data necessary for training models.\n\nMultiple Data Collection Methods: Users can collect data directly from devices, upload existing datasets, or even pull data from cloud storage solutions like Amazon S3.\nReal-Time Data Collection: The platform supports real-time data collection through phones, computers, or connected development boards. This versatility allows users to capture images, audio, and motion data using a web-based interface.\nData Explorer: This tool provides powerful data visualization capabilities, helping users to explore and understand their datasets. Enterprise users can also monitor model performance with new data and automate data processing, enhancing the platform’s scalability and efficiency.\nSynthetic Data Generation: While not fully tested, Edge Impulse offers tools to generate synthetic data, which can be valuable in scenarios where real-world data is scarce.\n\n\n\n\nEdge Impulse simplifies the machine learning process with a clear, step-by-step approach:\n\nImpulse Block Design: The platform’s ML workflow is divided into four block types: data extraction, data processing, training, and output. This modular approach ensures that users can easily follow the process and make adjustments as needed.\nDefault and Custom Processing Blocks: Users can choose from default data processing and training blocks or create custom models using tools like Keras. This flexibility is key for users with specific needs or those looking to experiment with novel approaches.\nComprehensive Visualization and Reporting: The platform provides a range of visualization tools, including feature importance analysis, confusion matrices, and performance profiles, which help users evaluate model accuracy and optimize performance.\n\n\n\n\nImpulse Block Design & Workflow\n\n\n\n\n\nOne of the standout features of Edge Impulse is its strong focus on edge devices:\n\nPerformance Profiling: The platform provides performance profiles for selected devices, detailing key metrics like RAM usage, disk space, and processing speed.\nDSP and Transformation Compilation: Users can compile digital signal processing (DSP) transformations and inference code specifically for their target devices, ensuring that models run efficiently on hardware with limited resources.\nHyperparameter Tuning: The platform offers tools for tuning model parameters and selecting the best-performing configurations for a given device, which is crucial for optimizing model deployment on resource-constrained environments."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#conclusion",
    "href": "blogs/edgeimplusestudio.html#conclusion",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse is a versatile, well-designed platform for developing and deploying machine learning models on edge devices. With its extensive feature set, modular workflow, and strong support for real-time data collection, it caters to both beginners and advanced users alike.\nFor more detailed information, visit the Edge Impulse homepage and Edge Impulse documentation."
  },
  {
    "objectID": "blogs/aicompressionsaas.html",
    "href": "blogs/aicompressionsaas.html",
    "title": "AI compression SaaS",
    "section": "",
    "text": "It has been nearly five years since I first wrote about TinyML and TinyML as-a-Service (TinyMLaaS) on the Ericsson blog.\n\nIn the rapidly evolving landscape of artificial intelligence (AI), efficiency and resource optimization are paramount. AI Compression as-a-Service (ACaaS) and TinyML as-a-Service (TinyMLaaS) emerge as transformative solutions that address the growing demand for deploying AI on resource-constrained devices. These services offer scalable, cost-effective, and high-performance AI capabilities, enabling a new wave of innovation in edge computing.\n\n\nAI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance.\n\n\n\n\nResource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security.\n\n\n\n\nTinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications.\n\n\n\n\n\n\nSmart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring.\n\n\n\n\nAI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "href": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "href": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Resource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "href": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "title": "AI compression SaaS",
    "section": "",
    "text": "TinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "href": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Smart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#conclusion",
    "href": "blogs/aicompressionsaas.html#conclusion",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/about.html",
    "href": "blogs/about.html",
    "title": "About",
    "section": "",
    "text": "What’s NinjaLABO is and how it was started!\n\n\n\nNinjaLABO’s origin",
    "crumbs": [
      "Get Started",
      "About"
    ]
  },
  {
    "objectID": "blogs/ericsson_blog2.html",
    "href": "blogs/ericsson_blog2.html",
    "title": "How can we democratize machine learning on IoT devices?",
    "section": "",
    "text": "ghttps://www.ericsson.com/en/blog/2020/2/how-can-we-democratize-machine-learning-iot-devices\n\n\n\nHow can we democratize machine learning on IoT devices?"
  },
  {
    "objectID": "blogs/tinymlaas.html",
    "href": "blogs/tinymlaas.html",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "href": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/HU_Summer_2023.html",
    "href": "blogs/HU_Summer_2023.html",
    "title": "TinyML as-a-Service with Helsinki University summer course 2023",
    "section": "",
    "text": "TinyMLaaS\n\n\nRead more…"
  }
]