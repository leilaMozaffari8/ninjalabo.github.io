[
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "Here, we visualize the performance of three runtimes: PyTorch, our custom-built tinyRuntime (both non-quantized and quantized versions), using the ResNet18 model and 100 images from the Imagenette dataset. We focus on four key metrics: accuracy, execution time, model size and memory usage.\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, save_image=False):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),\n        template=\"plotly_dark\", legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14),\n        margin=dict(t=90, b=60, l=80, r=50))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n    \n    if save_image:\n        fig.write_image(\"images/perf_bar.png\")\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86)\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on AMD64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.348224\n472.261719\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n10.044972\n84.859375\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n44.750036\n53.417969\n11.949406\n\n\n\n\n\n\n\n\n\nCode\ndef plot_radar(df, save_image=False):\n    # Normalize the data\n    df = df.copy()\n    df[\"Time\"] = df[\"Time\"].apply(lambda x: x / df[\"Time\"].max())\n    df[\"Accuracy\"] = df[\"Accuracy\"].apply(lambda x: x / 100)\n    df[\"Max memory\"] = df[\"Max memory\"].apply(lambda x: x / df[\"Max memory\"].max())\n    # Split based on runtime and take three relevant columns\n    dfs = [df_split[['Time', 'Max memory', 'Accuracy']] for df_split in split_dataframe(df)]\n    # Create a radar chart\n    categories = ['Time', 'Memory usage', 'Accuracy']\n    runtimes = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n    fig = go.Figure()\n    for i, df in enumerate(dfs):\n        fig.add_trace(go.Scatterpolar(\n            r=df.iloc[-1].values,\n            theta=categories,\n            fill='toself',\n            name=runtimes[i]\n        ))\n\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 1]\n            )\n        ),\n        font=dict(size=16),\n        margin=dict(t=50, b=50, l=50, r=50),\n        showlegend=True\n    )\n    fig.add_annotation(\n        text=\"*Values are normalized between 0 and 1\",\n        xref=\"paper\", yref=\"paper\",\n        x=0.5, y=-0.1,\n        showarrow=False,\n        font=dict(size=15)\n    )\n    fig.show()\n    \n    if save_image:\n        fig.write_image(\"images/perf_radar.png\")\n    \nplot_radar(df_x86)\n\n\n\n\n                                                \n\n\nFigure 2: Radar chart of time, memory, and accuracy for AMD64. Quantities are scaled between 0 and 1\n\n\n\n\n\n\n\n\n\nCode\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 3: Performance comparison on ARM64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n6.832752\n462.390625\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n2.672380\n143.281250\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n3.812798\n93.515625\n11.949406\n\n\n\n\n\n\n\n\n\nCode\nplot_radar(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 4: Radar chart of time, memory, and accuracy for ARM64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "performance.html#amd64",
    "href": "performance.html#amd64",
    "title": "Performance",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, save_image=False):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),\n        template=\"plotly_dark\", legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14),\n        margin=dict(t=90, b=60, l=80, r=50))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n    \n    if save_image:\n        fig.write_image(\"images/perf_bar.png\")\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86)\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on AMD64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.348224\n472.261719\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n10.044972\n84.859375\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n44.750036\n53.417969\n11.949406\n\n\n\n\n\n\n\n\n\nCode\ndef plot_radar(df, save_image=False):\n    # Normalize the data\n    df = df.copy()\n    df[\"Time\"] = df[\"Time\"].apply(lambda x: x / df[\"Time\"].max())\n    df[\"Accuracy\"] = df[\"Accuracy\"].apply(lambda x: x / 100)\n    df[\"Max memory\"] = df[\"Max memory\"].apply(lambda x: x / df[\"Max memory\"].max())\n    # Split based on runtime and take three relevant columns\n    dfs = [df_split[['Time', 'Max memory', 'Accuracy']] for df_split in split_dataframe(df)]\n    # Create a radar chart\n    categories = ['Time', 'Memory usage', 'Accuracy']\n    runtimes = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n    fig = go.Figure()\n    for i, df in enumerate(dfs):\n        fig.add_trace(go.Scatterpolar(\n            r=df.iloc[-1].values,\n            theta=categories,\n            fill='toself',\n            name=runtimes[i]\n        ))\n\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 1]\n            )\n        ),\n        font=dict(size=16),\n        margin=dict(t=50, b=50, l=50, r=50),\n        showlegend=True\n    )\n    fig.add_annotation(\n        text=\"*Values are normalized between 0 and 1\",\n        xref=\"paper\", yref=\"paper\",\n        x=0.5, y=-0.1,\n        showarrow=False,\n        font=dict(size=15)\n    )\n    fig.show()\n    \n    if save_image:\n        fig.write_image(\"images/perf_radar.png\")\n    \nplot_radar(df_x86)\n\n\n\n\n                                                \n\n\nFigure 2: Radar chart of time, memory, and accuracy for AMD64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "performance.html#arm64",
    "href": "performance.html#arm64",
    "title": "Performance",
    "section": "",
    "text": "Code\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 3: Performance comparison on ARM64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n6.832752\n462.390625\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n2.672380\n143.281250\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n3.812798\n93.515625\n11.949406\n\n\n\n\n\n\n\n\n\nCode\nplot_radar(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 4: Radar chart of time, memory, and accuracy for ARM64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "getstarted.html",
    "href": "getstarted.html",
    "title": "Get Started",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review",
    "crumbs": [
      "Get Started"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Optimizing a DNN Model for Processing Hyperspectral Imaging (HSI) Data\n\n\n\n\n\n\nTech\n\n\nDNN\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive Overview of DNN Model Compression Techniques\n\n\n\n\n\n\nTech\n\n\nDNN\n\n\n\n\n\n\n\n\n\nAug 18, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Real-Time Face Identification System: A Comprehensive Guide\n\n\n\n\n\n\nTech\n\n\nUseCase\n\n\n\nExplore how to build a real-time face identification system using Ultralytics YOLOv8, a state-of-the-art object detection model known for its speed and accuracy. We’ll compare various tools, including managed services like AWS Rekognition and Azure Face API, to help you choose the best solution for your needs. Whether you’re a developer seeking full control over your models or looking for an easy-to-integrate, scalable solution, this guide will provide you with the knowledge and tools to create a powerful face identification system.\n\n\n\n\n\nAug 18, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying PyTorch Static Quantization\n\n\n\n\n\n\nTech\n\n\n\nA deep dive into how PyTorch performs inference with quantized models.\n\n\n\n\n\nAug 12, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service with Helsinki University summer course 2023\n\n\n\n\n\n\nHU\n\n\nTinyMLaaS\n\n\n\nThis is the main repository of Tiny Machine Learning as a Service project for Software Engineering Project course at University of Helsinki, summer 2023.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAbout\n\n\n\n\n\n\nAbout\n\n\nTeam\n\n\n\nThe team of experts in software development, research, and design is working to enable businesses to integrate AI & Machine Learning applications on a wide range of devices without relying on expensive networking & cloud computing services thanks to their solution, TinyML as a Service.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAI Compression as-a-Service MVP6 review\n\n\n\n\n\n\nNews\n\n\nMVP\n\n\n\nOur internal review of MVP6 usage\n\n\n\n\n\nAug 6, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nEdge Impulse Review\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nCompetitor\n\n\n\nEdge Impulse - A Comprehensive Review of the Leading Edge AI Platform\n\n\n\n\n\nAug 5, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service Overview\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\n\nBrief overview of NinjaLABO’s TinyML as-a-Service / AI compression as-a-Service \n\n\n\n\n\nJul 17, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nIREE review\n\n\n\n\n\n\nTech\n\n\nCompiler\n\n\n\nBrief overview of IREE\n\n\n\n\n\nJul 12, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nImagimob Studio Review\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nCompetitor\n\n\n\nBrief overview of Imagimob Studio - A solution for Edge AI applications\n\n\n\n\n\nJul 8, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nAI compression SaaS\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\n\nBrief overview of AI Compression\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML Use Cases\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nUseCase\n\n\n\nUse cases for TinyML offered by NinjaLABO\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we democratize machine learning on IoT devices?\n\n\n\n\n\n\nEricsson\n\n\n\nMaking tiny machine learning widely available on edge IoT devices could prove to be a major leap in smart sensing across industries. Below, we plot the technical milestones to making that happen as part of our ongoing research into TinyML as-a-service.\n\n\n\n\n\nAug 6, 2020\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service and the challenges of machine learning at the edge\n\n\n\n\n\n\nEricsson\n\n\n\nThe TinyML as-a-Service project at Ericsson Research sets out to address the challenges that today limit the potential of machine learning (ML) paradigms at the edge of the embedded IoT world. In this post, the second post in our TinyML series, we take a closer look at the technical and non-technical challenges on our journey to making that happen. Learn more below.\n\n\n\n\n\nAug 6, 2019\n\n\nHiroshi Doyu\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Blog"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html",
    "href": "docs/tinymlaas/TinyML-backend_README.html",
    "title": "TinyML-backend",
    "section": "",
    "text": "GitHub Actions\n\n\nThis is the backend for the TinyMLaaS project.\n\n\nStart by cloning this repository. Then setup and run the app with the following steps.\n\n\n\nSetup venv with:\npython -m venv venv\nActivate the virtual environment with:\nsource venv/bin/activate\n\n\n\nInstall project dependencies by running the following command inside the virtual environment:\npip install -r requirements.txt\n\n\n\nIn a new terminal window, run the following commands to set up an sqlite database:\nsqlite3\n.open tiny_mlaas.db\n.read schema.sql\n.read populate.sql\nTo connect the backend to the database, create a .env file (in the project’s root directory) with the following line:\nDATABASE_URL=\"sqlite:///./tiny_mlaas.db\"\n\n\n\nThe projects main repository contains the machine learning modules. Download the modules by running the following command in the backend root directory:\nsvn checkout https://github.com/TinyMLaas/TinyMLaaS/trunk/TinyMLaaS_main\nOptionally clone the main repository and copy or symlink the TinyMLaaS_main folder into the backend’s root directory.\n\n\n\nWith the virtual environment activated in the project’s root directory, run the app with:\nuvicorn main:app --reload\n\n\n\nUse the application with the project frontend here, or explore the API by browsing to: BACKEND_URL/docs.\n\n\n\nRun unit tests in the backend root folder with:\npytest",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#instructions-for-running-the-application",
    "href": "docs/tinymlaas/TinyML-backend_README.html#instructions-for-running-the-application",
    "title": "TinyML-backend",
    "section": "",
    "text": "Start by cloning this repository. Then setup and run the app with the following steps.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#virtual-environment",
    "href": "docs/tinymlaas/TinyML-backend_README.html#virtual-environment",
    "title": "TinyML-backend",
    "section": "",
    "text": "Setup venv with:\npython -m venv venv\nActivate the virtual environment with:\nsource venv/bin/activate",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-backend_README.html#dependencies",
    "title": "TinyML-backend",
    "section": "",
    "text": "Install project dependencies by running the following command inside the virtual environment:\npip install -r requirements.txt",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#database-setup",
    "href": "docs/tinymlaas/TinyML-backend_README.html#database-setup",
    "title": "TinyML-backend",
    "section": "",
    "text": "In a new terminal window, run the following commands to set up an sqlite database:\nsqlite3\n.open tiny_mlaas.db\n.read schema.sql\n.read populate.sql\nTo connect the backend to the database, create a .env file (in the project’s root directory) with the following line:\nDATABASE_URL=\"sqlite:///./tiny_mlaas.db\"",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#machine-learning-modules-setup",
    "href": "docs/tinymlaas/TinyML-backend_README.html#machine-learning-modules-setup",
    "title": "TinyML-backend",
    "section": "",
    "text": "The projects main repository contains the machine learning modules. Download the modules by running the following command in the backend root directory:\nsvn checkout https://github.com/TinyMLaas/TinyMLaaS/trunk/TinyMLaaS_main\nOptionally clone the main repository and copy or symlink the TinyMLaaS_main folder into the backend’s root directory.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#run-app",
    "href": "docs/tinymlaas/TinyML-backend_README.html#run-app",
    "title": "TinyML-backend",
    "section": "",
    "text": "With the virtual environment activated in the project’s root directory, run the app with:\nuvicorn main:app --reload",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#usage",
    "href": "docs/tinymlaas/TinyML-backend_README.html#usage",
    "title": "TinyML-backend",
    "section": "",
    "text": "Use the application with the project frontend here, or explore the API by browsing to: BACKEND_URL/docs.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#testing",
    "href": "docs/tinymlaas/TinyML-backend_README.html#testing",
    "title": "TinyML-backend",
    "section": "",
    "text": "Run unit tests in the backend root folder with:\npytest",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html",
    "href": "docs/tinymlaas/TinyML-frontend_README.html",
    "title": "TinyML-frontend",
    "section": "",
    "text": "GitHub Actions\n\n\nfrontend for TinyMLaaS.\n\n\nDepends on package usbutils.\nOn Debian-based systems install it with:\napt install usbutils\n\n\n\nThe frontend uses the external command lsusb to find suitable usb-devices.\nThis means that the frontend can’t natively find usb devices on windows and needs to be run in a docker container for this feature.\n\n\n\nRun backend from this repository\nActivate virtual environment with:\nsource /venv/bin/activate\nInstall dependencies with:\npip install requirements.txt\nCreate an .env file in frontend root directory that points to backend:\nBACKEND_URL = \"http://localhost:8000\"\nRun frontend with:\nstreamlit run TinyMLaaS.py\n\n\n\nThis project uses Robot Framework to run end-to-end testing. For testing you need to have both the backend and frontend running. Before running frontend, environment variable ROBOT_TESTS should be set to true. On bash, you can do that with\nROBOT_TESTS=true && export ROBOT_TESTS\nThis makes it that the robot tests don’t access actual usb-devices, but rather use sepcifically defined mock data.\nIn the backend you will need to have enough test data in the database to run the robot tests. You can set up the database in the backend folder with\ntouch tiny_mlaas.db\nsqlite3 tiny_mlaas.db &lt; schema.sql\nsqlite3 tiny_mlaas.db &lt; populate.sql\nRun Robot Framework tests with:\nrobot -d robot_output tests/\nThe -d flag directs the robot test outputs, which can be quite generous, to a named folder.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#dependecies",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#dependecies",
    "title": "TinyML-frontend",
    "section": "",
    "text": "Depends on package usbutils.\nOn Debian-based systems install it with:\napt install usbutils",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#usb-detection",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#usb-detection",
    "title": "TinyML-frontend",
    "section": "",
    "text": "The frontend uses the external command lsusb to find suitable usb-devices.\nThis means that the frontend can’t natively find usb devices on windows and needs to be run in a docker container for this feature.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#running",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#running",
    "title": "TinyML-frontend",
    "section": "",
    "text": "Run backend from this repository\nActivate virtual environment with:\nsource /venv/bin/activate\nInstall dependencies with:\npip install requirements.txt\nCreate an .env file in frontend root directory that points to backend:\nBACKEND_URL = \"http://localhost:8000\"\nRun frontend with:\nstreamlit run TinyMLaaS.py",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#testing",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#testing",
    "title": "TinyML-frontend",
    "section": "",
    "text": "This project uses Robot Framework to run end-to-end testing. For testing you need to have both the backend and frontend running. Before running frontend, environment variable ROBOT_TESTS should be set to true. On bash, you can do that with\nROBOT_TESTS=true && export ROBOT_TESTS\nThis makes it that the robot tests don’t access actual usb-devices, but rather use sepcifically defined mock data.\nIn the backend you will need to have enough test data in the database to run the robot tests. You can set up the database in the backend folder with\ntouch tiny_mlaas.db\nsqlite3 tiny_mlaas.db &lt; schema.sql\nsqlite3 tiny_mlaas.db &lt; populate.sql\nRun Robot Framework tests with:\nrobot -d robot_output tests/\nThe -d flag directs the robot test outputs, which can be quite generous, to a named folder.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html",
    "href": "docs/tinymlaas/TinyML-MCU_README.html",
    "title": "TinyML-MCU",
    "section": "",
    "text": "Code for TinyMLaaS MCU devices.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#dependencies",
    "title": "TinyML-MCU",
    "section": "Dependencies",
    "text": "Dependencies\nThe bridge has a dependency on usbutils. On Debian-based systems, it can be installed with\napt install usbutils",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#running",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#running",
    "title": "TinyML-MCU",
    "section": "Running",
    "text": "Running\n\nLocally\nInstall the python dependecies\npip install -r requirements.txt\nThere are two different ways to run the bridge locally.\nFirst, to run a development server, run\nflask --app main.py --debug run\nFor production, this is not ideal. To use waitress as production server\nwaitress-serve main:app\n\n\nDocker\nThe docker version of the bridge requires nestybox/sysbox.\nThe provided docker-compose file looks like this\nversion: '3'\n\nservices:\n  bridge:\n    build:\n      context: .\n    # If you have devices connected to the relay, add them here\n    # devices:\n      # - \"/dev/ttyACM0:/dev/ttyACM0\"\n    volumes:\n      - \"/dev/bus/usb:/dev/bus/usb\"\n      - \"/dev/serial:/dev/serial\"\n    runtime: sysbox-runc\n    ports:\n      - 5000:8080\nAny microcontrollers you want to control with the bridge need to be added to the devices at this point. They also need to be connected to the computer.\nIf you want to add more microcontrollers to the bridge later, you can add them manually by adding them to the docker-compose file and restarting the container.\nThere is also a provided script to check for added Arduinos. The script docker-container-restarted automatically detects, when new arduinos are added and restart the container with the new devices. It can be run by making it executable and running it\nchmod u+x docker-container-restarter.py\n./docker-container-restarter.py\nThere might be problems with serial port permissions. As a solution, give permissions to the port for all users with\nchmod 777 /path/to/port\nNote that this will most likely require root priviledges.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#serving-the-application-to-internet",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#serving-the-application-to-internet",
    "title": "TinyML-MCU",
    "section": "Serving the application to internet",
    "text": "Serving the application to internet\n\nWebserver\nIf you already have a webserver setup, such as Apache2 or Nginx, with which you can serve the application to the internet, add a new proxy for the bridge according to the style of the webserver.\n\n\nPort forwarding\nAnother way is to add a new port forwarding setting to the host machine to port 5000 in the router.\n\n\nNgrok\nHowever, these might not be usable by everyone. With Ngrok, you can serve the bridge to the internet without having access to the local router.\n\nInstall ngrok\nServe the application with\n\nngrok http 5000",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#microcontrollers",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#microcontrollers",
    "title": "TinyML-MCU",
    "section": "Microcontrollers",
    "text": "Microcontrollers\nThere are seperate instructions for microcontrollers.\nArduino",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Use_cases.html",
    "href": "docs/tinymlaas/Use_cases.html",
    "title": "Use case scenarios",
    "section": "",
    "text": "As a security-conscious homeowner, I want to use TinyML(*) for human-detection to monitor my home and receive alerts when unexpected activity is detected, so that I can take action to ensure my home is safe.\n\nData Collection: User collects video footage from their smart camera, which will be used to train the TinyML model for human-detection.\nModel Training: User trains a TinyML human-detection model using the collected video footage and publicly available datasets.\nModel Squeezing: User optimizes the model size to ensure it can be deployed on TinyML-enabled devices.\nModel Deployment: User deploys the optimized TinyML model on their smart camera.\nInference: The smart camera uses the deployed TinyML model to perform real-time human-detection, identifying humans and activities within its field of view.\nAlerts: If the camera detects unusual activity, such as a person entering the home when no one is expected to be there, it sends alerts to the user’s smartphone.\nModel Update: User periodically updates the TinyML model with new data to ensure its accuracy and improve its performance over time.\nUser Access: User can access real-time video footage and receive notifications to confirm the alert and take appropriate action, such as contacting the police or checking in on the home from a remote location.\n\n(*) The alternative to TinyML would be running human detection in the cloud, with all the possible network latency issue we may face. TinyML-based human-detection can trigger an alarm installed “in place” where the camera is. By relying on cloud, we would need to transfer the video footage to the cloud and then running inference at there.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Use case scenarios"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Software-Project-Summer-Kick-off.html",
    "href": "docs/tinymlaas/Software-Project-Summer-Kick-off.html",
    "title": "Project starting point",
    "section": "",
    "text": "This was a successor of the previous software project in Winter. Here are the info:\n\nVideo Sprint3 demo, Mid\nVideo Sprint7 demo, Final\nSource code in GH Repo\nProject documentation, generated automatically\n\n\n\n\nimage.png\n\n\n     \n\nGoal for summer 2023 project\nRight now there’s no clear boundary between UI and its backend. We want to make them separted. The current Streamlit UI should be a pure frontend. The backend logic should be a REST backend server (e.g. fast API) Finall we want a CLI tool to control in additon to the current UI. A CLI tool should do the exact same things as the UI does right now.\n$ tmlaas device list\n&lt;list device name&gt;\n$ tmlaas model list\n&lt;list device name&gt;\n$ tmlass device=&lt;device id&gt; install model=&lt;model id&gt;\nYou may want to refer to this project as CLI example, https://ghapi.fast.ai/\n\n\nDevelopment environment\n\nSCRUM, User story mapping to set common goals with all stakeholders.\nNbdev, Jupyter notebook framework for code, (unit)tests & doc at once, Nbdev tutorial video.\nDocker compose to run the whole system at once, turorial video.\nAcceptance Test Driven Development (ATDD) to sync up with a client.\nStreamlit for UI framework used in this project.\nGH Project as Kanban\nGH Workflow as CI/CD\n\n\n\nCommunication\n\nDiscord, Click to join.\n\n\n\nNext\n\nWho’s SCRUM master for Sprint1?\nSprint1 (Week20)\n\nInitial research of this project by Students\n\nSprint1 review & planning at 10:00AM 22nd May\n\nReview WoW proposal from students\nQ&A for a client\nPrioritize user story?\n\nWhich Kanban board to share with customer?",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Project starting point"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html",
    "href": "docs/tinymlaas/Architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "This page contains general information about the architechture and how each component is related to each other.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#overview",
    "href": "docs/tinymlaas/Architecture.html#overview",
    "title": "Architecture",
    "section": "Overview",
    "text": "Overview\nTinyMLaaS consist of several components: - backend - streamlit frontend - cli - MCU components\nThe different components are stored in their own repositories which can be found here.\nThe backend is the core component which contains all the API endpoints. By calling them you can execute all the tasks necessary for the workflow. The backend is responsible with communicating with the machine learing components, storing data in the database and installing & managing MCU devices.\nTensorflow machine learning components live in the main repository and need to be fetched for the backend seperately.\nMCU repository contains the bridge for communicating with the devices and the code needed for devices.\nWe have implemented two different interfaces for the TinyMLaaS: CLI and website GUI using streamlit. Since you can make API calls directly to backend it’s extremely simple to build your own frontends in the future.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#block-diagram",
    "href": "docs/tinymlaas/Architecture.html#block-diagram",
    "title": "Architecture",
    "section": "Block Diagram",
    "text": "Block Diagram\n\n\n\nBlock Diagram\n\n\nThe backend is the main component that deals with calling the tensorflow functions and communicating with the MCU devices. Tensorflow is currently the supported UI but you can also make API calls directly or use the CLI. In the future the tensorflow components can be containarized as their own service.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#key-components",
    "href": "docs/tinymlaas/Architecture.html#key-components",
    "title": "Architecture",
    "section": "Key Components",
    "text": "Key Components\n\nML model training\nData Storage and loading (database)\nML model quantization and optimization\nML model compilation for MCUs",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#database-diagram",
    "href": "docs/tinymlaas/Architecture.html#database-diagram",
    "title": "Architecture",
    "section": "Database diagram",
    "text": "Database diagram\n\n\n\nDatabase Diagram",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "blogs/tinymlaas.html",
    "href": "blogs/tinymlaas.html",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "href": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/HU_Summer_2023.html",
    "href": "blogs/HU_Summer_2023.html",
    "title": "TinyML as-a-Service with Helsinki University summer course 2023",
    "section": "",
    "text": "TinyMLaaS\n\n\nRead more…"
  },
  {
    "objectID": "blogs/ericsson_blog2.html",
    "href": "blogs/ericsson_blog2.html",
    "title": "How can we democratize machine learning on IoT devices?",
    "section": "",
    "text": "ghttps://www.ericsson.com/en/blog/2020/2/how-can-we-democratize-machine-learning-iot-devices\n\n\n\nHow can we democratize machine learning on IoT devices?"
  },
  {
    "objectID": "blogs/dnn_hsi_opt.html",
    "href": "blogs/dnn_hsi_opt.html",
    "title": "Optimizing a DNN Model for Processing Hyperspectral Imaging (HSI) Data",
    "section": "",
    "text": "Introduction\nHyperspectral Imaging (HSI) is a powerful technique that captures a wide spectrum of light across dozens to hundreds of narrow wavelength bands. Unlike traditional RGB imaging, which only captures three broad bands of red, green, and blue, HSI provides rich spectral information for each pixel, enabling detailed analysis of the material properties, composition, and other characteristics of the observed objects.\nHowever, this richness comes at a cost: HSI data is massive and complex, requiring sophisticated processing techniques to extract useful information. Deep Neural Networks (DNNs) are a natural choice for processing such data due to their ability to model complex patterns and relationships. But given the high dimensionality and large data volumes inherent to HSI, optimizing DNNs for this task is crucial to ensure efficiency and effectiveness. This article explores the key strategies for optimizing DNN models tailored for HSI data.\n\n\n\n1. Dimensionality Reduction\nHSI datasets often contain hundreds of spectral bands, leading to high-dimensional data that can be computationally expensive to process and prone to overfitting. Dimensionality reduction techniques help mitigate these issues by reducing the number of input features while preserving the essential information.\n\nPrincipal Component Analysis (PCA): PCA is widely used to reduce the dimensionality of HSI data by transforming it into a set of orthogonal components that capture the most variance in the data. This reduces the input size for DNNs, making the models more manageable and less computationally intensive.\nLinear Discriminant Analysis (LDA): LDA is particularly useful in supervised learning tasks, as it maximizes class separability by finding the linear combinations of features that best separate the classes. This helps in focusing the DNN on the most discriminative features.\nNon-negative Matrix Factorization (NMF): NMF decomposes the data into non-negative components, making it particularly suitable for HSI data, which often consists of non-negative values. This method not only reduces dimensionality but also facilitates interpretability, as the resulting components often correspond to physically meaningful spectra.\n\n\n\n2. Custom Network Architectures\nGiven the unique characteristics of HSI data, custom DNN architectures can be designed to exploit both the spectral and spatial dimensions effectively.\n\n3D Convolutional Neural Networks (3D-CNNs): Unlike traditional 2D CNNs used for RGB images, 3D-CNNs process data across three dimensions: height, width, and spectral depth. This allows the model to simultaneously capture spatial patterns and spectral features, making it ideal for HSI data.\nSpectral-Spatial Networks: These networks separately handle spectral and spatial features before combining them. For example, 1D CNNs can be used to process spectral data, while 2D CNNs handle spatial data. This approach ensures that the model effectively captures relevant features from both domains.\nCapsule Networks: Capsule networks preserve the hierarchical relationships between features, which can be particularly useful when dealing with the complex and multidimensional nature of HSI data. They can enhance the model’s ability to understand spatial hierarchies and the orientation of spectral features.\n\n\n\n3. Lightweight Models and Compression Techniques\nDue to the large size and complexity of HSI data, DNN models can become computationally expensive to train and deploy. Techniques to reduce the model size and complexity are essential for making them practical, especially in resource-constrained environments.\n\nKnowledge Distillation: This technique involves training a smaller “student” model to mimic the predictions of a larger “teacher” model. The student model, while being less complex, learns to approximate the teacher’s performance, making it more efficient for processing HSI data.\nModel Compression: Compression techniques such as quantization, pruning, and low-rank approximation can significantly reduce the size of a DNN without drastically affecting its performance. For instance, pruning removes less important neurons or filters, reducing computational cost and memory usage.\nLightweight Architectures: Adapting lightweight models like MobileNet or EfficientNet for HSI data processing can significantly improve efficiency. These architectures are designed to be computationally efficient and can be customized to handle the higher dimensionality of HSI data.\n\n\n\n4. Data Augmentation and Regularization\nHSI datasets are often smaller compared to RGB datasets, which increases the risk of overfitting. Data augmentation and regularization are crucial for improving the generalization capability of DNN models.\n\nData Augmentation: Techniques such as spectral variation, spatial transformations, and noise injection can increase the diversity of the training data, helping the model to generalize better to unseen data. This is particularly important in HSI, where the data can be highly variable depending on the environmental conditions.\nRegularization Techniques: Methods like dropout, L2 regularization, and spectral regularization are important for preventing overfitting. Regularization helps the model to remain robust and generalize well, even when trained on high-dimensional HSI data.\n\n\n\n5. Multi-Scale Learning\nHSI data often contains features at multiple spatial or spectral scales, making multi-scale learning an effective approach.\n\nMulti-Scale CNNs: These networks use filters of different sizes to capture features at various scales, allowing the model to understand both fine-grained details and broader patterns in the HSI data. This is crucial for accurately capturing the complex relationships inherent in HSI data.\nAttention Mechanisms: Attention mechanisms enable the model to focus on the most relevant spectral bands or spatial regions, improving both efficiency and accuracy. By selectively focusing on important features, attention mechanisms help the model to prioritize the most critical information, reducing computational overhead.\n\n\n\nConclusion\nOptimizing DNN models for processing HSI data is essential due to the high dimensionality and complexity of the data. Through dimensionality reduction, custom architectures, model compression, data augmentation, regularization, and multi-scale learning, it is possible to develop efficient and effective models for HSI applications. These optimizations not only make HSI processing more practical but also open up new possibilities for deploying such models in real-world scenarios, including mobile and edge computing environments. As technology continues to advance, the integration of HSI with optimized DNNs will likely play a crucial role in a wide range of fields, from agriculture and environmental monitoring to healthcare and beyond."
  },
  {
    "objectID": "blogs/pytorch_staticq.html",
    "href": "blogs/pytorch_staticq.html",
    "title": "Demystifying PyTorch Static Quantization",
    "section": "",
    "text": "In the world of machine learning, optimizing model performance and efficiency is crucial, especially for deploying models on edge devices with limited resources. One powerful technique to achieve this is quantization, which reduces the precision of the numbers used in a model’s computations. PyTorch supports two types of quantization: dynamic and static. Dynamic quantization adjusts the precision of weights at runtime, while static quantization involves converting the model’s weights and activations to lower precision based on calibration data. This article will focus on statically quantized models, breaking down the core concepts and steps involved in PyTorch’s approach to inference with these models.\nNote: This article assumes you are already familiar with quantization, particularly static quantization. If not, I recommend checking out the some materials, e.g., our technology page for an introduction.\nfrom fastai.vision.all import *\n\nimport torch\nfrom torch.ao.quantization import get_default_qconfig_mapping\nimport torch.ao.quantization.quantize_fx as quantize_fx\nfrom torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\nLet’s start by creating a Quantizer class to quantize a PyTorch model. For an introduction to PyTorch quantization, you can refer to the official documentation. As an example, I will use the Imagenette2-320 dataset and the ResNet18 model. For convenience, I will leverage the Fastai learner to streamline this process.\nclass Quantizer():\n    def __init__(self, backend=\"x86\"):\n        self.qconfig = get_default_qconfig_mapping(backend)\n        torch.backends.quantized.engine = backend\n\n    def quantize(self, model, calibration_dls):\n        x, _ = calibration_dls.valid.one_batch()\n        model_prepared = prepare_fx(model.eval(), self.qconfig, x)\n        with torch.no_grad():\n            _ = [model_prepared(xb.to('cpu')) for xb, _ in calibration_dls.valid]\n\n        return model_prepared, convert_fx(model_prepared)\npath = untar_data(URLs.IMAGENETTE_320, data=Path.cwd()/'data')\ndls = ImageDataLoaders.from_folder(path, valid='val', item_tfms=Resize(224),\n                                   batch_tfms=Normalize.from_stats(*imagenet_stats))\nlearn = vision_learner(dls, resnet18)\nmodel_prepared, qmodel = Quantizer(\"qnnpack\").quantize(learn.model, learn.dls)\nIn static quantization, the scaling factors and zero points for weights and activations are determined after model calibration but before inference. In this context, we are using per-tensor quantization, which means that there is a single scaling factor and zero point applied uniformly across all elements in each tensor of a layer. This approach is straightforward and computationally efficient, as it simplifies the quantization process by treating the entire tensor as a whole.\nIn the above cell, model_prepared instance represents the model after it has recorded the range of activations across a validation dataset. This model contains the necessary information about the model structure and activation ranges, from which the scaling factors and zero points are calculated. Below is an example of the quantization parameters for some activations. The HistogramObserver is used to record the activation ranges. The first output shows the quantized parameters of the first activation, which is the model input, while the second output shows the quantization parameters of the second activation, which is the output of the first Conv2d + ReLU layer. In PyTorch, to avoid redundant quantization and dequantization processes between layers, batch normalization is folded into the preceding layer (batch normalization folding), and the ReLU layer is fused with the layer it follows.\n# Example activation quantization parameters\nfor i in range(3):\n    attr = getattr(model_prepared, f\"activation_post_process_{i}\")\n    scale, zero_p = attr.calculate_qparams()\n    print(\"{}\\nScaling Factor: {}\\nZero Point: {}\\n\".format(attr, scale.item(), zero_p.item()))\n\nHistogramObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\nScaling Factor: 0.018649335950613022\nZero Point: 114\n\nHistogramObserver(min_val=0.0, max_val=7.000605583190918)\nScaling Factor: 0.011327190324664116\nZero Point: 0\n\nHistogramObserver(min_val=0.0, max_val=7.000605583190918)\nScaling Factor: 0.011327190324664116\nZero Point: 0\nqmodel instance represents the quantized model. It contains quantized weights, along with their associated scaling factor and zero point, as well as the scaling factor and zero point for activations. Additionally, it includes some non-quantized parameters, which I will explain later.\nqmodel\n\nGraphModule(\n  (0): Module(\n    (0): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008901300840079784, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.024013830348849297, zero_point=149, padding=(1, 1))\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.007031331304460764, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.031252723187208176, zero_point=156, padding=(1, 1))\n      )\n    )\n    (5): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.007301042787730694, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.019116230309009552, zero_point=124, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.01664934679865837, zero_point=135)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.008282394148409367, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.02566305175423622, zero_point=137, padding=(1, 1))\n      )\n    )\n    (6): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.010484358295798302, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.02675902470946312, zero_point=90, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.008271278813481331, zero_point=162)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00832998938858509, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.027811763808131218, zero_point=142, padding=(1, 1))\n      )\n    )\n    (7): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.006999513134360313, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.023119885474443436, zero_point=140, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.02033478580415249, zero_point=128)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.006345659960061312, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.12105856835842133, zero_point=88, padding=(1, 1))\n      )\n    )\n  )\n  (1): Module(\n    (0): Module(\n      (mp): AdaptiveMaxPool2d(output_size=1)\n      (ap): AdaptiveAvgPool2d(output_size=1)\n    )\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): QuantizedDropout(p=0.25, inplace=False)\n    (4): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.08005672693252563, zero_point=0, qscheme=torch.per_tensor_affine)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): QuantizedDropout(p=0.5, inplace=False)\n    (8): QuantizedLinear(in_features=512, out_features=10, scale=0.10456003248691559, zero_point=150, qscheme=torch.per_tensor_affine)\n  )\n)\nLet’s investigate the first layer of qmodel, i.e., quantized Conv2d + ReLU layer.\nlayer = qmodel._modules['0']._modules['0']\nprint(layer)\nprint(\"Weight Scale: {}, Weight Zero Point: {}\".format(layer.weight().q_scale(),\n                                                       layer.weight().q_zero_point()))\nprint(\"Output Scaling Factor: {}, Output Zero Point: {}\\n\".format(layer.scale, \n                                                                  layer.zero_point))\n\nprint(\"Example weights:\", layer.weight()[0, 0, 0])\nprint(\"In integer representation:\", layer.weight()[0, 0, 0].int_repr())\n\nQuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\nWeight Scale: 0.0030892190989106894, Weight Zero Point: 0\nOutput Scaling Factor: 0.011327190324664116, Output Zero Point: 0\n\nExample weights: tensor([-0.0031,  0.0000,  0.0000,  0.0185,  0.0124,  0.0031, -0.0031],\n       size=(7,), dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n       scale=0.0030892190989106894, zero_point=0)\nIn integer representation: tensor([-1,  0,  0,  6,  4,  1, -1], dtype=torch.int8)\nAs shown above, the quantized layer contains two scaling factors and zero points: one for the weights and another for the output activation. You may have noticed that the output scaling factor and zero point are the same as those displayed in the second cell above, as they represent the same activation.\nWhat about biases, which I haven’t discussed yet? In PyTorch, biases are not quantized during the initial quantization stage. Instead, they are quantized at inference. Although bias quantization could technically be performed at the same stage as weight quantization, PyTorch does not display the quantized biases at this point. The formula for bias quantization in PyTorch is \\[\nb_q = round(b / (si * sw))\n\\] , where \\(b_q\\) is quantized bias, \\(b\\) is bias before quantization, \\(si\\) is input activation scale and \\(sw\\) is weight scale. For more details, you can refer to this discussion.\nIn addition, the model may include other non-quantized parameters, such as parameters in batch normalization layers that are not fused. This is likely because quantizing the activations in these layers would not provide significant benefits."
  },
  {
    "objectID": "blogs/pytorch_staticq.html#what-happens-during-inference",
    "href": "blogs/pytorch_staticq.html#what-happens-during-inference",
    "title": "Demystifying PyTorch Static Quantization",
    "section": "What happens during inference?",
    "text": "What happens during inference?\nThis section demonstrates how calculations are performed in the quantized model during inference. To illustrate this, I calculate the output of the first convolutional layer and validate it against the actual result.\n\nlayer_input = None\nlayer_output = None\n\ndef hook_fn(module, input, output):\n    global layer_output, layer_input\n    layer_input = input\n    layer_output = output\n\nimg = torch.rand([1, 3, 224, 224])\nhook = qmodel._modules['0']._modules['0'].register_forward_hook(hook_fn)\noutput = qmodel(img)\nhook.remove()\nprint(\"Example input:\", layer_input[0][0,0,0,:10].int_repr())\nprint(\"Example output:\", layer_output[0,0,0,:10].int_repr())\n\nExample input: tensor([163, 119, 155, 138, 126, 164, 115, 132, 115, 166], dtype=torch.uint8)\nExample output: tensor([10,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=torch.uint8)\n\n\n\nimport numpy as np\n\ndef quantize(x, qparams, itype):\n    xtype = torch.iinfo(itype)\n    return torch.clamp(torch.round(x / qparams[0]) + qparams[1], min=xtype.min, max=xtype.max)\n\ndef dequantize(x, qparams):\n    return (x - qparams[1]) * qparams[0]\n\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    N, C, H, W = input_data.shape\n    out_h = (H + 2 * pad - filter_h) // stride + 1\n    out_w = (W + 2 * pad - filter_w) // stride + 1\n\n    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride * out_h\n        for x in range(filter_w):\n            x_max = x + stride * out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n    return torch.tensor(col)\n\n# first use im2col, which is efficient way to perform Conv2d operation\ninp = im2col(img, 7, 7, 2, 3).float()\n# quantize input values using input scale and zero point\ninp = quantize(inp, [layer_input[0].q_scale(), layer_input[0].q_zero_point()], torch.uint8)\n# get quantized weights, weight scale and quantize biases\nw = qmodel._modules['0']._modules['0'].weight().int_repr().reshape(64, -1).float()\nsw = qmodel._modules['0']._modules['0'].weight().q_scale()\nb = quantize(qmodel._modules['0']._modules['0'].bias(),\n             [layer_input[0].q_scale() * sw, 0], torch.int32)\nb = b.reshape(1,64,1,1).detach()\n# calculate matmul in Conv2d and add biases\nout = (w @ (inp.T - layer_input[0].q_zero_point())).view(1,64,112,112) + b\n# dequantize, perform ReLU and quantize based on output scale and zero point\nout = out * sw * layer_input[0].q_scale()\nout = torch.relu(out)\nout = quantize(out, [layer_output.q_scale(), layer_output.q_zero_point()], torch.uint8)\n\n\ntorch.allclose(out, layer_output.int_repr().float())\nprint(\"Output: \", out[0, 0, 0, :10])\n\nOutput:  tensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\n\nOur calculation matches the actual result, which is a good sign. Although some operations in PyTorch’s implementation might be performed in a different order, the overall process is likely very similar."
  },
  {
    "objectID": "blogs/iree.html",
    "href": "blogs/iree.html",
    "title": "IREE review",
    "section": "",
    "text": "Compiling and optimizing ML models for multiple hardware platforms can be complex and time-consuming. One promising solution on our radar is the Intermediate Representation Execution Environment (IREE), which can significantly simplify this process. Here’s an in-depth look at what I’ve learned about IREE.\n\n\nIREE, or Intermediate Representation Execution Environment, is an end-to-end compiler designed specifically for machine learning (ML) models. It takes models expressed in an intermediate representation using Multi-Level Intermediate Representation (MLIR) and performs hardware-agnostic optimizations and transformations. Unlike traditional compilers, IREE doesn’t directly generate low-level machine code but uses various optimizers and accelerators to do this, making it a versatile framework that supports a variety of hardware platforms.\n\n\n\n\nImport Your Model: Begin by importing your machine learning model into IREE. This can be a model developed using one of the supported frameworks such as TensorFlow, PyTorch, JAX, ONNX, or TensorFlow Lite.\nConfigure Deployment Settings: Specify your deployment configuration, including the target platform (CPU, GPU, etc.), accelerators, and any other constraints relevant to your deployment environment.\nCompile Your Model: Utilize IREE to compile your model. During compilation, IREE optimizes the model’s code for the specified deployment configuration using its end-to-end compiler capabilities.\nRun Your Compiled Model: Once compiled, use IREE’s runtime components to execute your optimized model on the target hardware.\n\n\n\n\n\nIntermediate Representation (IR) and MLIR: IREE uses MLIR (Multi-Level Intermediate Representation) as its IR, which acts like a programming language for expressing machine learning models. This allows for sophisticated optimizations and transformations that are independent of the underlying hardware. IREE supports conversion from a variety of popular ML frameworks including JAX, ONNX, TensorFlow, TensorFlow Lite, and PyTorch into MLIR.\nAutomatic Optimization: Unlike traditional compilers, IREE integrates scheduling and execution logic during compilation, rather than deferring it to runtime. This approach reduces scheduling overhead and enhances efficiency by compiling optimized code tailored to the target hardware at compile time. Additionally, IREE excels in automatic code optimization through compilers it uses, for example parallelization and vectorization are performed automatically (in the case of CPU). This means it can automatically transform the input model code to leverage hardware capabilities effectively, ensuring efficient execution on different platforms.\nWide Hardware Support: IREE supports a broad range of hardware configurations, including various CPUs and GPUs. For example, it offers support for bare-metal, enabling deployment on edge devices with minimal footprint. With the help of the Hardware Abstraction Layer (HAL), IREE can compile for various hardware platforms on any supported machine. This versatility allows developers to optimize and deploy ML models across different hardware environments without the need for extensive platform-specific modifications.\nBindings IREE also provides bindings, which are interfaces that allow access to the IREE compiler and its components from different programming languages, such as Python. For example, you can compile and run models using IREE in the Python interface. This flexibility is crucial for integrating IREE into diverse workflows and leveraging its capabilities from various development environments.\n\n\n\n\nHere was an overview of IREE, emphasizing its capabilities in optimizing models across diverse hardware platforms. For practical examples, please visit IREE’s official documentation at https://iree.dev, where you can find comprehensive guides and examples illustrating how to compile ML models for various hardware configurations."
  },
  {
    "objectID": "blogs/iree.html#what-is-iree",
    "href": "blogs/iree.html#what-is-iree",
    "title": "IREE review",
    "section": "",
    "text": "IREE, or Intermediate Representation Execution Environment, is an end-to-end compiler designed specifically for machine learning (ML) models. It takes models expressed in an intermediate representation using Multi-Level Intermediate Representation (MLIR) and performs hardware-agnostic optimizations and transformations. Unlike traditional compilers, IREE doesn’t directly generate low-level machine code but uses various optimizers and accelerators to do this, making it a versatile framework that supports a variety of hardware platforms."
  },
  {
    "objectID": "blogs/iree.html#workflow-with-iree",
    "href": "blogs/iree.html#workflow-with-iree",
    "title": "IREE review",
    "section": "",
    "text": "Import Your Model: Begin by importing your machine learning model into IREE. This can be a model developed using one of the supported frameworks such as TensorFlow, PyTorch, JAX, ONNX, or TensorFlow Lite.\nConfigure Deployment Settings: Specify your deployment configuration, including the target platform (CPU, GPU, etc.), accelerators, and any other constraints relevant to your deployment environment.\nCompile Your Model: Utilize IREE to compile your model. During compilation, IREE optimizes the model’s code for the specified deployment configuration using its end-to-end compiler capabilities.\nRun Your Compiled Model: Once compiled, use IREE’s runtime components to execute your optimized model on the target hardware."
  },
  {
    "objectID": "blogs/iree.html#key-features-of-iree",
    "href": "blogs/iree.html#key-features-of-iree",
    "title": "IREE review",
    "section": "",
    "text": "Intermediate Representation (IR) and MLIR: IREE uses MLIR (Multi-Level Intermediate Representation) as its IR, which acts like a programming language for expressing machine learning models. This allows for sophisticated optimizations and transformations that are independent of the underlying hardware. IREE supports conversion from a variety of popular ML frameworks including JAX, ONNX, TensorFlow, TensorFlow Lite, and PyTorch into MLIR.\nAutomatic Optimization: Unlike traditional compilers, IREE integrates scheduling and execution logic during compilation, rather than deferring it to runtime. This approach reduces scheduling overhead and enhances efficiency by compiling optimized code tailored to the target hardware at compile time. Additionally, IREE excels in automatic code optimization through compilers it uses, for example parallelization and vectorization are performed automatically (in the case of CPU). This means it can automatically transform the input model code to leverage hardware capabilities effectively, ensuring efficient execution on different platforms.\nWide Hardware Support: IREE supports a broad range of hardware configurations, including various CPUs and GPUs. For example, it offers support for bare-metal, enabling deployment on edge devices with minimal footprint. With the help of the Hardware Abstraction Layer (HAL), IREE can compile for various hardware platforms on any supported machine. This versatility allows developers to optimize and deploy ML models across different hardware environments without the need for extensive platform-specific modifications.\nBindings IREE also provides bindings, which are interfaces that allow access to the IREE compiler and its components from different programming languages, such as Python. For example, you can compile and run models using IREE in the Python interface. This flexibility is crucial for integrating IREE into diverse workflows and leveraging its capabilities from various development environments."
  },
  {
    "objectID": "blogs/iree.html#conclusion",
    "href": "blogs/iree.html#conclusion",
    "title": "IREE review",
    "section": "",
    "text": "Here was an overview of IREE, emphasizing its capabilities in optimizing models across diverse hardware platforms. For practical examples, please visit IREE’s official documentation at https://iree.dev, where you can find comprehensive guides and examples illustrating how to compile ML models for various hardware configurations."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html",
    "href": "blogs/dnn_comp_techs.html",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Deep Neural Networks (DNNs) have achieved remarkable success across various domains, from image recognition to natural language processing. However, their deployment, especially in resource-constrained environments like mobile devices or edge computing, is often hindered by their size and computational demands. Model compression techniques are essential to address these challenges, enabling the use of DNNs in real-world applications without sacrificing too much performance. This article provides a comprehensive overview of the major DNN model compression techniques, presenting a big picture of how each approach contributes to making neural networks more efficient.\n\n\n\nQuantization reduces the precision of the numbers representing model parameters, thus decreasing the memory and computational requirements. There are several quantization approaches and methods, each of which can be used independently or in combination to optimize model performance.\nQuantization Approaches: - Post-Training Quantization (PTQ): This technique converts the weights and activations of a pre-trained model to lower precision (e.g., from 32-bit floating point to 8-bit integers) without requiring further training. - Quantization-Aware Training (QAT): Unlike PTQ, QAT involves training the model with simulated quantization effects, allowing the network to learn to be robust to the lower precision. This typically results in better performance compared to PTQ. Quantization Methods: - Static Quantization: Quantizes weights and activations using a representative dataset. All required quantization parameters are calculated beforehand, making inference faster compared to dynamic quantization. - Dynamic Quantization: Applies quantization to activations dynamically during inference. This method adapts to varying data more effectively than static quantization, providing flexibility for data with a wide range of values. - Weight-Only Quantization: Focuses on quantizing only the model weights while keeping activations at a higher precision. This approach can help maintain model accuracy compared to quantizing both weights and activations, but it results in a larger memory footprint. - Group quantization: Quantizes weights in predefined groups separately, allowing more precise control and potentially better trade-offs between accuracy and memory size. - Symmetric vs. Asymmetric Quantization: Symmetric quantization uses a single scale for both positive and negative values, while asymmetric quantization allows addionally zero-points (shifting values), , fully utilizing the quantization range. Symmetric quantization is generally faster, whereas asymmetric quantization offers improved accuracy. - Mixed Precision Quantization: Utilizes different precision levels (e.g., 16-bit floating point, 8-bit integers) within the same model. This method selects precision dynamically based on the importance of different layers or operations and can be combined with various quantization techniques to optimize performance and efficiency.\nEach quantization method can be used in combination, depending on the specific needs of the model and deployment scenario. By effectively applying these techniques, you can achieve a balance between model accuracy, computational efficiency, and resource usage.\n\n\n\nPruning involves removing less important parameters (e.g., weights, neurons, filters) from the network, effectively “trimming” the model without significant loss in performance. Pruning can be categorized into:\n\nUnstructured Pruning: Removes individual weights based on certain criteria, such as magnitude-based pruning, where weights with the smallest magnitude are removed. This type of pruning can lead to sparse matrices that are harder to accelerate on standard hardware.\n\nLottery Ticket Hypothesis: Suggests that within a large network, there exist smaller subnetworks (“winning tickets”) that can be trained to achieve performance similar to the original network.\n\nStructured Pruning: In contrast to unstructured pruning, this method removes entire structures, such as filters or channels, making the resulting model more hardware-friendly.\n\nFilter Pruning: Removes entire filters in convolutional layers.\nChannel Pruning: Prunes entire channels across feature maps.\nBlock Pruning: Removes blocks of weights or layers.\nAutomated Gradual Pruning: Gradually removes parameters during training based on a predefined schedule, as implemented in tools like FasterAI.\n\nDynamic Pruning: Adjusts the pruning strategy during inference or training based on runtime conditions, ensuring the model adapts to changing computational constraints.\n\n\n\n\nKnowledge Distillation is a technique where a smaller “student” model is trained to mimic the behavior of a larger “teacher” model. The student model learns not only from the labeled data but also from the soft predictions of the teacher, which provides richer information about the data.\n\nTeacher-Student Distillation: The classic approach where the student model is trained using the outputs of a pre-trained teacher model.\n\nSoft Targets: The student model is trained to match the teacher’s softened outputs (probabilities) rather than the hard labels.\nIntermediate Layer Distillation: The student learns from the intermediate representations of the teacher model, not just the final output.\n\nSelf-Distillation: A single model distills knowledge from itself by using its own predictions from previous training iterations as targets.\nCross-Model Distillation: Distillation occurs between different types of models, such as distilling from an ensemble of models to a single student model.\n\n\n\n\nLow-Rank Factorization reduces the dimensionality of the model parameters by decomposing matrices or tensors into products of lower-dimensional entities:\n\nMatrix Factorization:\n\nSingular Value Decomposition (SVD): Decomposes weight matrices into products of smaller matrices, effectively reducing the model size.\n\nTensor Factorization:\n\nCP Decomposition: Decomposes tensors (multi-dimensional arrays) into sums of outer products of vectors.\nTucker Decomposition: Generalizes matrix decomposition to higher dimensions by decomposing a tensor into a core tensor multiplied by matrices along each mode.\n\n\n\n\n\nBatchNorm Folding combines Batch Normalization layers with preceding layers to reduce the computational load during inference. BatchNorm layers are merged with the preceding convolutional or fully connected layers during inference, reducing the number of operations. This is not an approximation; it means you can achieve model compression without sacrificing accuracy.\n\n\n\nWeight Sharing is a technique used in neural networks to reduce the number of parameters, thus improving computational efficiency and potentially enhancing generalization. The core idea is to use the same weights across different parts of the network, which can lead to significant reductions in model size and complexity.\n\n\n\nNeural Architecture Search automates the design of neural networks, optimizing them for specific constraints, such as size, speed, or accuracy:\n\nReinforcement Learning-Based NAS: Uses reinforcement learning to explore different architectures.\nEvolutionary Algorithm-Based NAS: Applies evolutionary strategies to evolve network architectures over successive generations.\nGradient-Based NAS: Utilizes gradients to guide the search for optimal architectures.\nHardware-Aware NAS: Tailors the search to optimize architectures specifically for target hardware, balancing performance with computational efficiency."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#quantization",
    "href": "blogs/dnn_comp_techs.html#quantization",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Quantization reduces the precision of the numbers representing model parameters, thus decreasing the memory and computational requirements. There are several quantization approaches and methods, each of which can be used independently or in combination to optimize model performance.\nQuantization Approaches: - Post-Training Quantization (PTQ): This technique converts the weights and activations of a pre-trained model to lower precision (e.g., from 32-bit floating point to 8-bit integers) without requiring further training. - Quantization-Aware Training (QAT): Unlike PTQ, QAT involves training the model with simulated quantization effects, allowing the network to learn to be robust to the lower precision. This typically results in better performance compared to PTQ. Quantization Methods: - Static Quantization: Quantizes weights and activations using a representative dataset. All required quantization parameters are calculated beforehand, making inference faster compared to dynamic quantization. - Dynamic Quantization: Applies quantization to activations dynamically during inference. This method adapts to varying data more effectively than static quantization, providing flexibility for data with a wide range of values. - Weight-Only Quantization: Focuses on quantizing only the model weights while keeping activations at a higher precision. This approach can help maintain model accuracy compared to quantizing both weights and activations, but it results in a larger memory footprint. - Group quantization: Quantizes weights in predefined groups separately, allowing more precise control and potentially better trade-offs between accuracy and memory size. - Symmetric vs. Asymmetric Quantization: Symmetric quantization uses a single scale for both positive and negative values, while asymmetric quantization allows addionally zero-points (shifting values), , fully utilizing the quantization range. Symmetric quantization is generally faster, whereas asymmetric quantization offers improved accuracy. - Mixed Precision Quantization: Utilizes different precision levels (e.g., 16-bit floating point, 8-bit integers) within the same model. This method selects precision dynamically based on the importance of different layers or operations and can be combined with various quantization techniques to optimize performance and efficiency.\nEach quantization method can be used in combination, depending on the specific needs of the model and deployment scenario. By effectively applying these techniques, you can achieve a balance between model accuracy, computational efficiency, and resource usage."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#pruning",
    "href": "blogs/dnn_comp_techs.html#pruning",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Pruning involves removing less important parameters (e.g., weights, neurons, filters) from the network, effectively “trimming” the model without significant loss in performance. Pruning can be categorized into:\n\nUnstructured Pruning: Removes individual weights based on certain criteria, such as magnitude-based pruning, where weights with the smallest magnitude are removed. This type of pruning can lead to sparse matrices that are harder to accelerate on standard hardware.\n\nLottery Ticket Hypothesis: Suggests that within a large network, there exist smaller subnetworks (“winning tickets”) that can be trained to achieve performance similar to the original network.\n\nStructured Pruning: In contrast to unstructured pruning, this method removes entire structures, such as filters or channels, making the resulting model more hardware-friendly.\n\nFilter Pruning: Removes entire filters in convolutional layers.\nChannel Pruning: Prunes entire channels across feature maps.\nBlock Pruning: Removes blocks of weights or layers.\nAutomated Gradual Pruning: Gradually removes parameters during training based on a predefined schedule, as implemented in tools like FasterAI.\n\nDynamic Pruning: Adjusts the pruning strategy during inference or training based on runtime conditions, ensuring the model adapts to changing computational constraints."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#knowledge-distillation",
    "href": "blogs/dnn_comp_techs.html#knowledge-distillation",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Knowledge Distillation is a technique where a smaller “student” model is trained to mimic the behavior of a larger “teacher” model. The student model learns not only from the labeled data but also from the soft predictions of the teacher, which provides richer information about the data.\n\nTeacher-Student Distillation: The classic approach where the student model is trained using the outputs of a pre-trained teacher model.\n\nSoft Targets: The student model is trained to match the teacher’s softened outputs (probabilities) rather than the hard labels.\nIntermediate Layer Distillation: The student learns from the intermediate representations of the teacher model, not just the final output.\n\nSelf-Distillation: A single model distills knowledge from itself by using its own predictions from previous training iterations as targets.\nCross-Model Distillation: Distillation occurs between different types of models, such as distilling from an ensemble of models to a single student model."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#low-rank-factorization",
    "href": "blogs/dnn_comp_techs.html#low-rank-factorization",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Low-Rank Factorization reduces the dimensionality of the model parameters by decomposing matrices or tensors into products of lower-dimensional entities:\n\nMatrix Factorization:\n\nSingular Value Decomposition (SVD): Decomposes weight matrices into products of smaller matrices, effectively reducing the model size.\n\nTensor Factorization:\n\nCP Decomposition: Decomposes tensors (multi-dimensional arrays) into sums of outer products of vectors.\nTucker Decomposition: Generalizes matrix decomposition to higher dimensions by decomposing a tensor into a core tensor multiplied by matrices along each mode."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#batchnorm-folding",
    "href": "blogs/dnn_comp_techs.html#batchnorm-folding",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "BatchNorm Folding combines Batch Normalization layers with preceding layers to reduce the computational load during inference. BatchNorm layers are merged with the preceding convolutional or fully connected layers during inference, reducing the number of operations. This is not an approximation; it means you can achieve model compression without sacrificing accuracy."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#weight-sharing",
    "href": "blogs/dnn_comp_techs.html#weight-sharing",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Weight Sharing is a technique used in neural networks to reduce the number of parameters, thus improving computational efficiency and potentially enhancing generalization. The core idea is to use the same weights across different parts of the network, which can lead to significant reductions in model size and complexity."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#neural-architecture-search-nas",
    "href": "blogs/dnn_comp_techs.html#neural-architecture-search-nas",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Neural Architecture Search automates the design of neural networks, optimizing them for specific constraints, such as size, speed, or accuracy:\n\nReinforcement Learning-Based NAS: Uses reinforcement learning to explore different architectures.\nEvolutionary Algorithm-Based NAS: Applies evolutionary strategies to evolve network architectures over successive generations.\nGradient-Based NAS: Utilizes gradients to guide the search for optimal architectures.\nHardware-Aware NAS: Tailors the search to optimize architectures specifically for target hardware, balancing performance with computational efficiency."
  },
  {
    "objectID": "blogs/about.html",
    "href": "blogs/about.html",
    "title": "About",
    "section": "",
    "text": "What’s NinjaLABO is and how it was started!\n\n\n\nNinjaLABO’s origin",
    "crumbs": [
      "Get Started",
      "About"
    ]
  },
  {
    "objectID": "blogs/howto_enable_jupyter_remotely.html",
    "href": "blogs/howto_enable_jupyter_remotely.html",
    "title": "How to set up Jupyter notebook on a remote VM",
    "section": "",
    "text": "ssh root@gpu-instance-ip\njupyter notebook --no-browser --port=8888 --allow-root\nssh -N -L localhost:7777:localhost:8888 root@gpu-instance-ip\nhttps://www.scaleway.com/en/docs/tutorials/setup-jupyter-notebook/"
  },
  {
    "objectID": "blogs/imagimobstudio.html",
    "href": "blogs/imagimobstudio.html",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is an end-to-end platform designed for developing Edge AI and Machine Learning applications. The platform covers the machine learning workflow from data collection to model deployment in embedded devices to support both experts and non-experts in building production grade models.\n\n\n\n\nImagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C.\n\n\n\n\n\n\nPredictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars.\n\n\n\n\nImagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/imagimobstudio.html#key-features",
    "href": "blogs/imagimobstudio.html#key-features",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C."
  },
  {
    "objectID": "blogs/imagimobstudio.html#use-cases",
    "href": "blogs/imagimobstudio.html#use-cases",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Predictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars."
  },
  {
    "objectID": "blogs/imagimobstudio.html#conclusion",
    "href": "blogs/imagimobstudio.html#conclusion",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/aicompressionsaas.html",
    "href": "blogs/aicompressionsaas.html",
    "title": "AI compression SaaS",
    "section": "",
    "text": "It has been nearly five years since I first wrote about TinyML and TinyML as-a-Service (TinyMLaaS) on the Ericsson blog.\n\nIn the rapidly evolving landscape of artificial intelligence (AI), efficiency and resource optimization are paramount. AI Compression as-a-Service (ACaaS) and TinyML as-a-Service (TinyMLaaS) emerge as transformative solutions that address the growing demand for deploying AI on resource-constrained devices. These services offer scalable, cost-effective, and high-performance AI capabilities, enabling a new wave of innovation in edge computing.\n\n\nAI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance.\n\n\n\n\nResource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security.\n\n\n\n\nTinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications.\n\n\n\n\n\n\nSmart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring.\n\n\n\n\nAI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "href": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "href": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Resource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "href": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "title": "AI compression SaaS",
    "section": "",
    "text": "TinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "href": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Smart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#conclusion",
    "href": "blogs/aicompressionsaas.html#conclusion",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/tinymlusecases.html",
    "href": "blogs/tinymlusecases.html",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "In today’s competitive landscape, every company is leveraging AI or exploring its integration into their business operations. As AI models become increasingly sophisticated, the operational expenditures (OPEX) associated with utilizing expensive GPUs in cloud datacenters also rise. Moreover, large AI models often cannot be executed on small devices without relying on cloud GPUs.\nNinjaLABO’s AI model compression (TinyML as-a-Service) addresses these challenges by offering versatile solutions applicable across various industries. Below, we explore specific focus areas and use cases where these solutions are particularly relevant:\n\n\n\nData Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage.\n\n\n\n\n\n\nData Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy.\n\n\n\n\n\n\nData Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security.\n\n\n\n\n\n\nData Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications.\n\n\n\n\n\n\nData Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications.\n\n\n\n\n\n\nData Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "href": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage."
  },
  {
    "objectID": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "href": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy."
  },
  {
    "objectID": "blogs/tinymlusecases.html#industrial-automation",
    "href": "blogs/tinymlusecases.html#industrial-automation",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security."
  },
  {
    "objectID": "blogs/tinymlusecases.html#agriculture",
    "href": "blogs/tinymlusecases.html#agriculture",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#automotive-and-mobility",
    "href": "blogs/tinymlusecases.html#automotive-and-mobility",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#security-and-surveillance",
    "href": "blogs/tinymlusecases.html#security-and-surveillance",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html",
    "href": "blogs/edgeimplusestudio.html",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse has rapidly become a prominent player in the field of machine learning for edge devices. Tailored to cater to a wide range of users, from hobbyists to professional embedded engineers, Edge Impulse Studio offers a comprehensive platform that supports the development, training, and deployment of machine learning models directly onto edge devices.\n\n\nEdge Impulse Studio is a cloud-based platform designed to simplify the modeling and deployment process of machine learning models on edge devices. It’s structured to accommodate a variety of user levels, from beginners exploring ML for the first time to seasoned professionals who require advanced features for enterprise applications. The platform supports multiple subscription tiers: Community, Professional, and Enterprise, each unlocking different levels of functionality and support.\nThe architecture of Edge Impulse is modular and flexible, allowing users to integrate different data sources, customize processing pipelines, and deploy models on various devices. The platform is organized into “impulse blocks” that guide users through the ML workflow, from data acquisition to model deployment. This block-based approach ensures that each step is transparent and manageable, especially for users with lower level of expertise.\n\n\n\nEdge Impulse general architecture & integrations with existing ML tools\n\n\n\n\n\n\n\nEdge Impulse offers a robust suite of tools for data collection and management, making it easy to gather the data necessary for training models.\n\nMultiple Data Collection Methods: Users can collect data directly from devices, upload existing datasets, or even pull data from cloud storage solutions like Amazon S3.\nReal-Time Data Collection: The platform supports real-time data collection through phones, computers, or connected development boards. This versatility allows users to capture images, audio, and motion data using a web-based interface.\nData Explorer: This tool provides powerful data visualization capabilities, helping users to explore and understand their datasets. Enterprise users can also monitor model performance with new data and automate data processing, enhancing the platform’s scalability and efficiency.\nSynthetic Data Generation: While not fully tested, Edge Impulse offers tools to generate synthetic data, which can be valuable in scenarios where real-world data is scarce.\n\n\n\n\nEdge Impulse simplifies the machine learning process with a clear, step-by-step approach:\n\nImpulse Block Design: The platform’s ML workflow is divided into four block types: data extraction, data processing, training, and output. This modular approach ensures that users can easily follow the process and make adjustments as needed.\nDefault and Custom Processing Blocks: Users can choose from default data processing and training blocks or create custom models using tools like Keras. This flexibility is key for users with specific needs or those looking to experiment with novel approaches.\nComprehensive Visualization and Reporting: The platform provides a range of visualization tools, including feature importance analysis, confusion matrices, and performance profiles, which help users evaluate model accuracy and optimize performance.\n\n\n\n\nImpulse Block Design & Workflow\n\n\n\n\n\nOne of the standout features of Edge Impulse is its strong focus on edge devices:\n\nPerformance Profiling: The platform provides performance profiles for selected devices, detailing key metrics like RAM usage, disk space, and processing speed.\nDSP and Transformation Compilation: Users can compile digital signal processing (DSP) transformations and inference code specifically for their target devices, ensuring that models run efficiently on hardware with limited resources.\nHyperparameter Tuning: The platform offers tools for tuning model parameters and selecting the best-performing configurations for a given device, which is crucial for optimizing model deployment on resource-constrained environments.\n\n\n\n\n\nEdge Impulse is a versatile, well-designed platform for developing and deploying machine learning models on edge devices. With its extensive feature set, modular workflow, and strong support for real-time data collection, it caters to both beginners and advanced users alike.\nFor more detailed information, visit the Edge Impulse homepage and Edge Impulse documentation."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#platform-overview",
    "href": "blogs/edgeimplusestudio.html#platform-overview",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse Studio is a cloud-based platform designed to simplify the modeling and deployment process of machine learning models on edge devices. It’s structured to accommodate a variety of user levels, from beginners exploring ML for the first time to seasoned professionals who require advanced features for enterprise applications. The platform supports multiple subscription tiers: Community, Professional, and Enterprise, each unlocking different levels of functionality and support.\nThe architecture of Edge Impulse is modular and flexible, allowing users to integrate different data sources, customize processing pipelines, and deploy models on various devices. The platform is organized into “impulse blocks” that guide users through the ML workflow, from data acquisition to model deployment. This block-based approach ensures that each step is transparent and manageable, especially for users with lower level of expertise.\n\n\n\nEdge Impulse general architecture & integrations with existing ML tools"
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#key-features",
    "href": "blogs/edgeimplusestudio.html#key-features",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse offers a robust suite of tools for data collection and management, making it easy to gather the data necessary for training models.\n\nMultiple Data Collection Methods: Users can collect data directly from devices, upload existing datasets, or even pull data from cloud storage solutions like Amazon S3.\nReal-Time Data Collection: The platform supports real-time data collection through phones, computers, or connected development boards. This versatility allows users to capture images, audio, and motion data using a web-based interface.\nData Explorer: This tool provides powerful data visualization capabilities, helping users to explore and understand their datasets. Enterprise users can also monitor model performance with new data and automate data processing, enhancing the platform’s scalability and efficiency.\nSynthetic Data Generation: While not fully tested, Edge Impulse offers tools to generate synthetic data, which can be valuable in scenarios where real-world data is scarce.\n\n\n\n\nEdge Impulse simplifies the machine learning process with a clear, step-by-step approach:\n\nImpulse Block Design: The platform’s ML workflow is divided into four block types: data extraction, data processing, training, and output. This modular approach ensures that users can easily follow the process and make adjustments as needed.\nDefault and Custom Processing Blocks: Users can choose from default data processing and training blocks or create custom models using tools like Keras. This flexibility is key for users with specific needs or those looking to experiment with novel approaches.\nComprehensive Visualization and Reporting: The platform provides a range of visualization tools, including feature importance analysis, confusion matrices, and performance profiles, which help users evaluate model accuracy and optimize performance.\n\n\n\n\nImpulse Block Design & Workflow\n\n\n\n\n\nOne of the standout features of Edge Impulse is its strong focus on edge devices:\n\nPerformance Profiling: The platform provides performance profiles for selected devices, detailing key metrics like RAM usage, disk space, and processing speed.\nDSP and Transformation Compilation: Users can compile digital signal processing (DSP) transformations and inference code specifically for their target devices, ensuring that models run efficiently on hardware with limited resources.\nHyperparameter Tuning: The platform offers tools for tuning model parameters and selecting the best-performing configurations for a given device, which is crucial for optimizing model deployment on resource-constrained environments."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#conclusion",
    "href": "blogs/edgeimplusestudio.html#conclusion",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse is a versatile, well-designed platform for developing and deploying machine learning models on edge devices. With its extensive feature set, modular workflow, and strong support for real-time data collection, it caters to both beginners and advanced users alike.\nFor more detailed information, visit the Edge Impulse homepage and Edge Impulse documentation."
  },
  {
    "objectID": "blogs/mvp6.html",
    "href": "blogs/mvp6.html",
    "title": "AI Compression as-a-Service MVP6 review",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review"
  },
  {
    "objectID": "blogs/ericsson_blog1.html",
    "href": "blogs/ericsson_blog1.html",
    "title": "TinyML as-a-Service and the challenges of machine learning at the edge",
    "section": "",
    "text": "https://www.ericsson.com/en/blog/2019/12/tinyml-as-a-service\n\n\n\nTinyML as-a-Service and the challenges of machine learning at the edge"
  },
  {
    "objectID": "blogs/yolofaceid.html",
    "href": "blogs/yolofaceid.html",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "",
    "text": "In today’s world, face identification systems are becoming increasingly crucial in various applications, from security and surveillance to personalized customer experiences. Building a real-time face identification system can seem daunting, but with the right tools and approaches, it becomes manageable. In this article, we will walk you through a project to develop such a system, using some of the best tools available, including Ultralytics YOLOv8, managed cloud services (e.g. AWS Rekognition and Azure Face API), and more. We’ll also compare different technologies to help you choose the best one for your needs."
  },
  {
    "objectID": "blogs/yolofaceid.html#setting-up-yolov8",
    "href": "blogs/yolofaceid.html#setting-up-yolov8",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Setting Up YOLOv8",
    "text": "Setting Up YOLOv8\n\nInstall the Ultralytics Package: Begin by installing the necessary software using pip:\npip install ultralytics\nLoad the YOLOv8 Model: Load a pre-trained YOLOv8 model or start with a custom model:\nfrom ultralytics import YOLO\n\n# Load a YOLOv8 model\nmodel = YOLO('yolov8n.pt')\n\n# Perform inference on an image\nresults = model('path/to/your/image.jpg')\nresults.show()  # Display results"
  },
  {
    "objectID": "blogs/yolofaceid.html#training-on-a-custom-dataset",
    "href": "blogs/yolofaceid.html#training-on-a-custom-dataset",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Training on a Custom Dataset",
    "text": "Training on a Custom Dataset\nTo fine-tune YOLOv8 for face detection, you need a dataset of face images with annotations in YOLO format. You can use public datasets like WIDER FACE or CelebA.\n\nPrepare Your Dataset: Ensure your dataset is properly annotated and structured.\nConfigure and Train the Model: Train YOLOv8 using your dataset:\nmodel = YOLO('yolov8n.pt')\nmodel.train(data='path/to/data.yaml', epochs=100, imgsz=640, batch=16)"
  },
  {
    "objectID": "blogs/yolofaceid.html#deploying-for-real-time-face-detection",
    "href": "blogs/yolofaceid.html#deploying-for-real-time-face-detection",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Deploying for Real-Time Face Detection",
    "text": "Deploying for Real-Time Face Detection\nOnce trained, you can deploy YOLOv8 for real-time face detection. This can be done by feeding live video streams into the model and processing each frame.\nimport cv2\nfrom ultralytics import YOLO\n\n# Load the trained model\nmodel = YOLO('runs/train/exp/weights/best.pt')\n\n# Capture video from webcam\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Perform inference\n    results = model(frame)\n\n    # Display results\n    results.show()\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "blogs/yolofaceid.html#key-factors",
    "href": "blogs/yolofaceid.html#key-factors",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Key Factors:",
    "text": "Key Factors:\n\nGPU is superiour to CPU at inference from cost vs perf comparison.\nProcessing Power: Each GPU’s ability to handle a specific number of frames per second (FPS).\nTotal FPS Requirement: The total computational load imposed by the 200 cameras.\nRack-Mount Server Specifications: The number of GPUs each server can support."
  },
  {
    "objectID": "blogs/yolofaceid.html#calculating-total-fps-requirement",
    "href": "blogs/yolofaceid.html#calculating-total-fps-requirement",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Calculating Total FPS Requirement:",
    "text": "Calculating Total FPS Requirement:\n\n200 Cameras at 30 FPS each:\n\nTotal FPS required = 200 cameras * 30 FPS = 6000 FPS."
  },
  {
    "objectID": "blogs/yolofaceid.html#gpu-capacity",
    "href": "blogs/yolofaceid.html#gpu-capacity",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "GPU Capacity:",
    "text": "GPU Capacity:\n\nNVIDIA RTX 3080: Can handle approximately 400 FPS for YOLOv8-tiny and around 80-100 FPS for standard YOLOv8.\nNVIDIA RTX 3090: Can handle approximately 600 FPS for YOLOv8-tiny and around 120-150 FPS for standard YOLOv8.\nNVIDIA RTX 4090: Can handle approximately 1000 FPS for YOLOv8-tiny and around 200-250 FPS for standard YOLOv8."
  },
  {
    "objectID": "blogs/yolofaceid.html#example-server-configurations",
    "href": "blogs/yolofaceid.html#example-server-configurations",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Example Server Configurations:",
    "text": "Example Server Configurations:\n\nSupermicro 4U GPU Server (SYS-4029GP-TRT):\n\nSupports up to 4 GPUs.\nIf each server uses 4x RTX 4090 GPUs, the total processing capability per server would be:\n\nYOLOv8-tiny: 4000 FPS (4 GPUs * 1000 FPS).\nStandard YOLOv8: 800-1000 FPS (4 GPUs * 200-250 FPS).\n\n\nASUS ESC8000A-E11:\n\nSupports up to 8 GPUs.\nWith 8x RTX 4090 GPUs:\n\nYOLOv8-tiny: 8000 FPS (8 GPUs * 1000 FPS).\nStandard YOLOv8: 1600-2000 FPS (8 GPUs * 200-250 FPS)."
  },
  {
    "objectID": "blogs/yolofaceid.html#number-of-servers-needed",
    "href": "blogs/yolofaceid.html#number-of-servers-needed",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Number of Servers Needed:",
    "text": "Number of Servers Needed:\n\nFor YOLOv8-tiny:\n\nWith Supermicro 4U Server: 1.5 servers (round up to 2 servers).\nWith ASUS ESC8000A-E11: 1 server is sufficient.\n\nFor Standard YOLOv8:\n\nWith Supermicro 4U Server: 6-8 servers.\nWith ASUS ESC8000A-E11: 3-4 servers."
  },
  {
    "objectID": "blogs/yolofaceid.html#summary",
    "href": "blogs/yolofaceid.html#summary",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "SUMMARY",
    "text": "SUMMARY\n\nYOLOv8-tiny: You would need 1-2 servers (depending on GPU configuration) to handle the 200 cameras.\nStandard YOLOv8: You would need 3-8 servers depending on the specific GPU and server model you choose.\n\nThese estimates assume maximum utilization of each GPU’s capabilities, and the actual number might vary based on real-world conditions, such as CPU bottlenecks, memory limitations, and other factors."
  },
  {
    "objectID": "blogs/yolofaceid.html#key-parameters",
    "href": "blogs/yolofaceid.html#key-parameters",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Key Parameters",
    "text": "Key Parameters\n\nResolution:\n\n2MP (1080p): Approximately 3 Mbps\n4MP: Approximately 6 Mbps\n\nFrame Rate: 30 frames per second (FPS)\nRecording Time: 24 hours per day, 30 days per month\nNumber of Cameras: 200\nCompression: H.264 assumed (H.265 could reduce storage by up to 50%)"
  },
  {
    "objectID": "blogs/yolofaceid.html#storage-calculation",
    "href": "blogs/yolofaceid.html#storage-calculation",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Storage Calculation",
    "text": "Storage Calculation\nThe total storage required can be calculated using the following equation:\n\\[\n\\text{Total Storage (TB)} = \\left(\\frac{\\text{Bitrate (Mbps)} \\times 86,400 \\times 30 \\times \\text{Number of Cameras}}{8 \\times 1024^2}\\right)\n\\]\nWhere:\n\n86,400: Number of seconds in a day\n30: Number of days in a month\n8: Conversion factor from bits to bytes\n1024^2: Conversion from MB to TB"
  },
  {
    "objectID": "blogs/yolofaceid.html#results",
    "href": "blogs/yolofaceid.html#results",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Results",
    "text": "Results\n\nFor 2MP Cameras (3 Mbps):\n\nTotal Storage: Approximately 185.2 TB per month.\n\nFor 4MP Cameras (6 Mbps):\n\nTotal Storage: Approximately 370.5 TB per month."
  },
  {
    "objectID": "blogs/yolofaceid.html#considerations",
    "href": "blogs/yolofaceid.html#considerations",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Considerations",
    "text": "Considerations\n\nCompression: Using H.265 compression could potentially reduce the storage requirement by 50%.\nMotion Detection: If motion detection is used instead of continuous recording, the required storage could be significantly less."
  },
  {
    "objectID": "blogs/yolofaceid.html#summary-1",
    "href": "blogs/yolofaceid.html#summary-1",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "SUMMARY",
    "text": "SUMMARY\nFor 200 cameras recording continuously at 2-4MP resolution, the storage requirement ranges from approximately 185.2 TB to 370.5 TB per month. Adjustments to compression methods, frame rates, or recording strategies could alter these estimates."
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html",
    "href": "docs/tinymlaas/Demonstration.html",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "",
    "text": "This document will demonstrate the steps in TinyMLaaS app. Some pages have dependencies on other pages, so go through pages from top to bottom is recommended at the start.\nIn order to run TinyMLaaS end-to-end following components need to be run: - The Frontend - The Backend - The Relay\nThese can all easily be started with the help of docker using the docker-compose-with-bridge.yml file in the Main repository. The application can be started with",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#device",
    "href": "docs/tinymlaas/Demonstration.html#device",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Device",
    "text": "Device\nIn order to install a model to a embedded device, the briging device of the wanted device and the device need to be selected.\nThe first thing that should be done on the Device page is to either add a new bridge or selecting an existing bridge. Let’s add the bridge that was started by docker compose. To do that, add the name of the bridge docker container with the port 8080 as the bridges address. The bridge will also not use a HTTPS connection in this case.\n\n\n\nAdd new bridge\n\n\nAfter adding the bridge, select the wanted bridge by clicking the Select bridge button next to the wanted bridge\n\n\n\nSelect the bridge\n\n\nSelecting a device to which to install the trained machine learning model later on is required. If the wanted device has not been registered already, register it either manually or by selecting it from the list of devices connected to the bridge. Lets add a device connected to the bridge by pressing the Register this device button next to that device\n\n\n\nRegister the new device\n\n\nAdd the missing information on the form and click add\n\n\n\nDevice form\n\n\nThe added device will automatically be selected as the active device.\n\n\n\nSelected device",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#data",
    "href": "docs/tinymlaas/Demonstration.html#data",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Data",
    "text": "Data\nIn order to train a model, a dataset with which to train the model needs to be selected.\n\n\n\nDataset selection complete\n\n\nUser can add images from local storage to selected dataset.\nIf the existing datasets are not enough, a new dataset can be added to the software.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#model",
    "href": "docs/tinymlaas/Demonstration.html#model",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Model",
    "text": "Model\nThis page shows already trained models as well as allows training of a new model.\nLet’s train a new model. For this, we first need to decide the parameters with which to train the model with. This time we chose to train the model with 27 epochs and with a batch size of 56. The image size is 96x96, as this model is trained for an Arduino, which takes pictures of this size.\n\n\n\nTrain a new model\n\n\nAfter the training is done, the software will show an image of the statistics of the training process as well as a test image with a prediction that the newly trained model gave for that picture.\n\n\n\nAfter training",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#compiling",
    "href": "docs/tinymlaas/Demonstration.html#compiling",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Compiling",
    "text": "Compiling\nThe page is responsible for ML compilation. It will turn the selected ML model and turn it into a tflite model as well as generate a C-array of it. The C array is the tflite model turned into bytes stored in a C array, which is required for embedded devices, which do not have a filesystem.\nAfter the compiling is done, the newly compiled model will be selected as the active model.\n\n\n\nCompilation done",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#installing",
    "href": "docs/tinymlaas/Demonstration.html#installing",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Installing",
    "text": "Installing\nNow that a model has been compiled, it can be installed on the device that was selected on the Device page. The page shows a single button, install. When this is pressed, the software will install the selected compiled model to the selected device on the selected bridge.\nBe sure that the software has access to the device. If you are not sure, the next command will give all users permissions to read, write and execute to the machine\nchmod 777 /path/to/port\nThis time, the device is connected to /dev/ttyACM0, so it was given permissions.\nNow, install the model to the device.\n\n\n\nInstall successfully complete",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#observing",
    "href": "docs/tinymlaas/Demonstration.html#observing",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Observing",
    "text": "Observing\nOn the observing page, user can see real-time predictions from device when the start button has been activated.\n\n\n\nReal-time predictions as device output",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html",
    "href": "docs/tinymlaas/TinyMLaaS_README.html",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "This is the main repository of Tiny Machine Learning as a Service project for Software Engineering Project course at University of Helsinki, summer 2023.\n\n\n\nGitHub Actions\n\n\n\n\nThe GitHub pages describe the overview of the project and the functionality of the machine learning modules: training, compiling, installing and observing.\n\n\n\n\nBackend\nFrontend\nCLI\nMCU components\n\n\n\n\n\nWay of Working\nProduct backlog\nWorking hours\nDatabase schema\n\n\n\n\nUse Docker to build and run the whole project.\n\nClone this repository\nRun\n\ndocker compose up -d\nThis will set up both the backend and frontend, and a network between the two\nIf the bridge is also needed on the same machine, use the docker-compose-with-bridge.yml file. This will build and run the frontend, backend and bridge and create a network between all three components.\n\nClone this repository\nRun\n\ndocker compose up -f docker-compose-with-brdige.yml -d\nNote that requires the Sysbox runtime to be installed and running, as the bridge uses this module.\n\n\nSee instructions in respective repositories for frontend, backend and the birdge / relay service for MCUs.\n\nBackend\nFrontend\nMCUs\nCLI",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#overview",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#overview",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "The GitHub pages describe the overview of the project and the functionality of the machine learning modules: training, compiling, installing and observing.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#repositories",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#repositories",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Backend\nFrontend\nCLI\nMCU components",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#documentation",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#documentation",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Way of Working\nProduct backlog\nWorking hours\nDatabase schema",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#running-the-project",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#running-the-project",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Use Docker to build and run the whole project.\n\nClone this repository\nRun\n\ndocker compose up -d\nThis will set up both the backend and frontend, and a network between the two\nIf the bridge is also needed on the same machine, use the docker-compose-with-bridge.yml file. This will build and run the frontend, backend and bridge and create a network between all three components.\n\nClone this repository\nRun\n\ndocker compose up -f docker-compose-with-brdige.yml -d\nNote that requires the Sysbox runtime to be installed and running, as the bridge uses this module.\n\n\nSee instructions in respective repositories for frontend, backend and the birdge / relay service for MCUs.\n\nBackend\nFrontend\nMCUs\nCLI",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html",
    "href": "docs/tinymlaas/Background.html",
    "title": "Background information and some literature sources",
    "section": "",
    "text": "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. TinyFedTL is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\nWhile recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. On-device training has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#federated-learning-in-tinyml",
    "href": "docs/tinymlaas/Background.html#federated-learning-in-tinyml",
    "title": "Background information and some literature sources",
    "section": "",
    "text": "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. TinyFedTL is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\nWhile recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. On-device training has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#edge-impulse",
    "href": "docs/tinymlaas/Background.html#edge-impulse",
    "title": "Background information and some literature sources",
    "section": "Edge Impulse",
    "text": "Edge Impulse\nEdge Impulse is the leading development platform for ML on edge devices. It is free for developers. With Edge Impulse one can: - build advanced embedded ML apps - build custom datasets rapidly - collect sensor, audio or camera data from devices, files or cloud integrations - use automatic labeling tools such as object detection and audio segmentation - use Edge Impulse cloud infrastructure to set up and run reusable scripted operations that transform the input data on large sets of data in parallel - integrate deployment pipelines, CI/CD tools and custom data sources with open APIs - develop models and algortihms - with prebuilt DSP (Digital Signal Processors) and ML blocks - hardware decisions can be made on device performance and Flash/RAM on every step - DSP feature extraction algorithms can be customized - custom machine learning models with Keras APIs - use visualized insights on datasets, model performance and memory to fine-tune the production model - optimize models and algorithms - EON TUNER for finding balance between DSP configurations and model architecture, budgeted against memory and latency constraints - EON Compiler for lighter and faster neural networks with equal accuracy - have full visibility across the whole ML pipeline - complete access to data attributes, DSP algorithms, model hyperparameters throughout whole development lifecycle - test model performance accurately - virtual cloud hardware simulation framework to get performance and accuracy metrics before deploying on any physical device - model performance can be evaluated with live classification - devices, automated ML pipeline testing, integration with the testing framework - deploy easily on any edge target - optimize source code by generating optimized embedded libraries and applications for any edge device - build ready-to-go binaries with selected development boards supported by Edge Impulse with special firmware - without OS or hardware dependencies and compile to nearly anything - make digital twin by re-deploying cloud-hosted ML projects to any hardware target on the fly - benefit from access and integrations to the leading hardware partner ecosystem from MCUs to MPUs and GPUs including acceleration - Arduino - Himax - OpenMV - Nvidia - Nordic semiconductor - Raspberry Pi - Silicon Labs - Sony - ST - Syntiant - Texas Instruments",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#arduino-and-wiring",
    "href": "docs/tinymlaas/Background.html#arduino-and-wiring",
    "title": "Background information and some literature sources",
    "section": "Arduino and Wiring",
    "text": "Arduino and Wiring\nArduino is an open-source electronics platform intended for anyone to make interactive projects. It is based on easy-to-use hardware and software. Arduino boards can - read inputs: light on a sensor, finger on a button, twitter message - turn inputs into outputs: activate motor, turn on a LED, publish something online - be instructed what to do by sending a set of instructions to the microcontroller on the board - using Arduino programming language (based on Wiring) and the Arduino Software (IDE) (based on Processing, an open source integrated development environment (IDE) like the Arduino IDE)\nBecause of simple and accessible user experience Arduino has been used in thousands of projects and applications by a worldwide community of makers, students, hobbyists, artists, programmers and professionals to produce a vast amount of accessible knowledge. The product range includes products for IoT applications, wearable, 3D printing and embedded environments.\nThe Arduino software - is easy-to-use for beginners but flexible enough for advanced users - runs on Mac, Windows and Linux - is used by teachers and students to build low cost scientific instruments to prove chemistry and physics principles and to get started with programming and robotics - is used by designers and architects to build interactive prototypes - is used by musicians and artists to build installations ansd to experiment with new musical instruments - is used by makers to build projects - can be used by anyone by following detailed instructions of a kit or sharing ideas online\nArduino offers some pros over other microcontrollers and microcontroller platforms available: - inexpensive compared to other microcontrolle platforms; the least expensive version of the Arduino module can be assembled by hand, pre-assembled Arduino modules are also affordable - cross-platform, the Arduino software (IDE) runs on Windows, Macintosh OSX, Linux (most microcontroller systems are limited to Windows) - simple, clear programming environment - easy-to-use for beginners but flexible enough for advanced users - because it’s based on Processing programming environment, students learning to program in that environment will be familiar with how the Arduino IDE works - open source and extensible software, published as open source tools - the language is based on AVR-C programming and can be expanded through C++ libraries - AVR-C code can be added directly into the Arduino programs - open source and extensible hardware, with the plans of the Arduino boards published under a Creative Commons licence, so own versions of the module with extensions and improvements can be built by anyone\nCode can be developed in the Arduino Cloud to build smart IoT projects. - smart devices can be connected within minutes - wide range of compatible devices, the Arduino Cloud provides the necessary code - nice dashbords can be created with mix and match customizable widgets to visualize real time or historical data or to control the device - projects can be controlled from anywhere in the world from any device, for example Alexa. - projects and libraries are always synced and up to date - all projects are cloud based and accessible from any device - data is always ecrypted and always belongs to the user (you) - is open and customizable, has flexible APIs to integrate and customize Cloud - all connected Arduino boards have a built-in crypto chip that makes them incredibly secure - sketches and project data are stored in AES 256-bit encrypted datastores - account security is protected with single use authentication codes - open and transparent data privacy terms and your data always belongs to you - has a wide range of resources - tutorials - APIs - documentation\nArduino’s history is interesting. The Arduino project was based on the developing platform Wiring created by Hernando Barragán as a Master’s thesis project at the Interaction Design Institute Ivrea (IDII) in Ivrea, Italy in 2003.\nHernando has been developing Wiring ever since and now Wiring is an open source electronics prototyping platform composed of programming language, an integrated development environment (IDE) , and a single-board microcontroller. More about Wiring can be found here.\nWiring offers new boards to customers in their webshop, but Wiring supports other boards directly, so Wiring IDE can be used on other boards as well (and more will be added). Thus Arduino board can be used with the Wiring IDE. Wiring can be downloaded for Linux, MacOS X and Windows.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#mbed",
    "href": "docs/tinymlaas/Background.html#mbed",
    "title": "Background information and some literature sources",
    "section": "Mbed",
    "text": "Mbed\nMbed is an open-source platform and operating system for internet-connected devices that are based on 32-bit ARM Cortex-M microcontrollers. These devices are known as IoT devices. Mbed is collaboratively developed by Arm and its tech partners.\nMbed is free and open source IoT operating system with connectivity, security, storage, device management and ML. Mbed offers free development tools, thousands of code examples and support for hundreds of microcontrollers development boards, as described here. - Mbed has its own Mbed OS with - well-defined API to develop C++ applications - free tools and thousands of code examples, libraries and drivers for common components - built-in security stack - core components such as storage and connectivity options - Mbed Enabled hardware has many options - Compiler and IDE - Keil Studio Cloud - modern, zero-installation Mbed development environment in the browser - code high-lighting, WebUSB flash and debug and version control - Mbed Studio - an IDE for application and library development - single environment with everything to create, compile and debug Mbed programs - Mbed CLI - command line interface allows to integrate Mbed functionality into preferred editor or enhance automation setup - Security - Arm Mbed TLS provides comprehensive SSL/TLS solution - easy to include cryptographic and SSL/TLS capabilities in the software and embedded products - as an SSL library Arm Mbed TLS provides an intuitive API, readable source code and a minimal and highly configurable code footprint",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#ifttt",
    "href": "docs/tinymlaas/Background.html#ifttt",
    "title": "Background information and some literature sources",
    "section": "IFTTT",
    "text": "IFTTT\nIFTTT is a private company that runs online digital automation platforms and offers them as a service. IFTTT is short for If This Then That. IFTTT integrates apps, devices and services quick and easy. IFTTT makes tech incompatibility easy to tackle. Automating process is simple, the user chooses - trigger - action(s) - name for the applet and finish\nIFTTT has over 700 services ready (more added weekly) to be automated. Price range of the services is from 0€/forever to 5€/month. IFTTT provides a simple way to create for example a smart home: - make a user account and log in (can be done with Google or Facebook) - trigger: give Google Assistant a voice command “Hey Google, I need coffee” - action(s): coffee machine is turned on and when the coffee is ready, the coffee machine turns off - name the applet: “make coffee” and finish the applet",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#sony-mesh",
    "href": "docs/tinymlaas/Background.html#sony-mesh",
    "title": "Background information and some literature sources",
    "section": "Sony MESH",
    "text": "Sony MESH\nSony MESH is a Sony corporate startup that sells a range of colored blocks with different sensors and wireless connection to the IoT. It’s digital DIY platform to connect everyday objects into IoT and create your own projects. - MESH blocks are wireless - Visual Coding App called Canvas simplifies programming and wiring with drag-and-drop functions - project can be connected to web services and popular smart gadgets like WeMo and Google Assistant voice activation - hardware projects can be expanded without expertise - IoT block in the project allows additions of smart features such as motion-sensitivity, remote control, orientation monitoring, voice commands, notifications, text messaging and more - projects can be connected to the internet instantly - project can be transformed into an IoT device, such as - Twitter alarm system - a voice-activated, data-logging, remote-controlled car - allows customization of smart gadgets - MESH is compatible with over 350 smart gadgets, home automation devices and web services on IFTTT - each IoT block has built-in IFTTT integration, so that it’s simple to add custom features on a smart gadget - MESH Motion and MESH Temperature & Humidity used together allow addition of motion-activated, multi-room temperature monitoring to a smart device like Nest thermostat - allows to build own smart gadget - MESH GPIO is a simple interface for development boards like Arduino and Raspberry Pi or actuators like a DC motor - MESH GPIO integrates any smart devices or web services on IFTTT, incl. - Amazon Alexa for Echo - Google Assistant - Google Sheets - LIFX - Nest - Phillips Hue - Twitter - WeMo - over 350 more - MESH blocks use Bluetooth - MESH blocks are rechargeable, durable and compact",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#sources",
    "href": "docs/tinymlaas/Background.html#sources",
    "title": "Background information and some literature sources",
    "section": "Sources:",
    "text": "Sources:\nOn-Device Training of Machine Learning Models on Microcontrollers with Federated Learning\nTinyFedTL: Federated Transfer Learning on Ubiquitous Tiny IoT Devices\nEdge Impulse\nBeginner’s Guide to DSP\nKeras APIs\nArduino\nArduino Getting started guide\nArduino Tutorials on Arduino Project Hub\nMbed\nMbed (product)\nIFTTT\nMESH blocks\nWiring\nWiring webshop",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html",
    "href": "docs/tinymlaas/Next_steps.html",
    "title": "Suggestions for further development",
    "section": "",
    "text": "The TinyMLaaS and TinyML-backend repositories both contain a branch called dockerize_tensorflow. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running.\nCompiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress.\nThe feature works with Docker in Docker (dind) using the nestybox/sysbox runtime environment which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\nThe dockerize_tensorflow branch contains a Dockerfile that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS dockerize_tensorflow branch uses backend_no_tf.Dockerfile to pull the dockerize_tensorflow branch from the TinyML-backend repo.\nOur FastAPI backend communicates with the docker image and containers using Docker API and more specifically the Docker SDK for Python. The feature is implemented in the tf_docker/compile.py backend module.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#work-in-progress-isolate-tensorflow-from-backend",
    "href": "docs/tinymlaas/Next_steps.html#work-in-progress-isolate-tensorflow-from-backend",
    "title": "Suggestions for further development",
    "section": "",
    "text": "The TinyMLaaS and TinyML-backend repositories both contain a branch called dockerize_tensorflow. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running.\nCompiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress.\nThe feature works with Docker in Docker (dind) using the nestybox/sysbox runtime environment which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\nThe dockerize_tensorflow branch contains a Dockerfile that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS dockerize_tensorflow branch uses backend_no_tf.Dockerfile to pull the dockerize_tensorflow branch from the TinyML-backend repo.\nOur FastAPI backend communicates with the docker image and containers using Docker API and more specifically the Docker SDK for Python. The feature is implemented in the tf_docker/compile.py backend module.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#database",
    "href": "docs/tinymlaas/Next_steps.html#database",
    "title": "Suggestions for further development",
    "section": "Database",
    "text": "Database\n\nProduction ready database\nThe backend uses SQLite. However, as SQlite is so lightweight, there are drawbacks, that affect the usage of the software. First of all, SQLite is not meant for storing big files. Because of this, all datasets, models and compiled models are stored outside the database in directories, and the database contains the path to the files. This is not ideal, as the backend can get messy with all the directories and if permanent storage is required outside the docker container, all of these volumes need to be mounted to the docker container.\nA SQLite database is also a single file, meaning that accidentally deleting the database or misplacing it is more common.\nBecause of these drawbacks, we would suggest changing the used database from SQLite to something more robust and production ready, such as MariaDB or PostgreSQL. Larger files can be stored in these databases and it is easy to mount one database rather than multiple different locations for all different saving locations.\n\n\nMove saving of models and datasets to database\nAs mentioned in the previous section, all datasets and models are stored outside the database. With the change of the database, it would be better to save all these files in the database.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#datasets",
    "href": "docs/tinymlaas/Next_steps.html#datasets",
    "title": "Suggestions for further development",
    "section": "Datasets",
    "text": "Datasets\n\nDownloading/uploading datasets (as zip)\nCurrently, you can only upload and append to datasets. It’s not possible to view or download it anyway. The application should have way to download the entire dataset to your machine (propably as zip so it works on both Windows and Linux).\nAt the moment you can only send bulk of pictures of to the app to create a dataset. It’s propably good idea to allow sending zips and creating datasets from them. You propably need to make sure that - it’s a valid dataset with working images - the saved dataset keeps same folder structure as the send zip - the user doesn’t send malicious data\n\n\nEditing, deleting and managing datasets\nCurrently, only adding and appending datasets is supported. The user should propably be able to manage the datasets better. First you should be able to remove unwanted datasets. Second, the user should be able to make folder structure for the datasets and edit them for labeling purposes see this tutorial for example how the data is structured. Being able to simply download the datasets,editing it locally, removing it from the app and readding it allows for the user to edit it. After that, if you want to enhance the user experience one idea is to add better editing options to the app itself: Being able remove photos, being able to add photos, creating new folders, removing folders.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#cli",
    "href": "docs/tinymlaas/Next_steps.html#cli",
    "title": "Suggestions for further development",
    "section": "CLI",
    "text": "CLI\nThe CLI is mostly autogenerated with this OpenAPI Generator. The current generator works as far as the existing API endpoints stay the same regarding input and output. Whenever new HTTP methods are added or modified a new generation has to be done. This can be done by using the json-file provided by the FastAPI backend. See detailed instructions here.\n\nAdd missing functionality + fixes to existing\nOnly some of the functionality is available with the CLI. See examples from the already implemented functionality from services and tiny_mlaas.py. To help with implementing more functionality, see examples for using the client from the docs. For example for bridges here.\nAdding and listing existing devices isn’t working properly because newly added devices don’t have a designated bridge. It’s worth considering if adding a device should be possible without a bridge. This fix would be implemented in backend. Alternative fix is to modify the validation done by the generated client.\nTraining via the CLI trains a model, but the output contains a picture that can’t be displayed in CLI. A possible fix is to modify the training function so that displaying an example of a prediction is optional.\nMany of the functions have hardcoded parameters. This helps with development and testing, but should be fixed in the future. See example of adding parameters: https://github.com/TinyMLaas/TinyML-CLI/blob/main/services/models.py . Here dataset_id and description are required parameters. Epoch and the rest of the hard coded variables can be handled in a similar fashion.\n\n\nAutogenerate end-to-end CLI from OpenAPI YAML\nThe current version of the CLI is autogenerated with the exception of services and tiny_mlaas.py.\nEnd-to-end autogeneration could be done after publishing services and tiny-mlaas.py as a Python package and having it as a dependency. See instructions: https://typer.tiangolo.com/tutorial/package/.\nA preliminary idea for implementing the end-to-end generation is following:\n\nPublish a package that contains the forementioned files\nOpenAPI yaml-file is needed for generating the CLI tool. Get it by browsing to backend_url/openapi.json. Convert the json to yaml with for example: https://editor.swagger.io/\n\nThe repository for autogeneration would consist of the yaml -file from step 2 and requirements.txt -file with the package published in step 1 (in addition to current requirements). With these prerequisites the steps for end-to-end generation could be:\n\nClone repo\nInstall the generator tool with: npm install @openapitools/openapi-generator-cli -g\nGenerate the client with npx @openapitools/openapi-generator-cli generate -i file.yaml -g python -o output_path\n\nfile.yaml is the yaml-file in the repo\noutput path should probably be the root directory of the cloned CLI-repository\n\nInstall the requirements: pip install -r requirements.txt\nFinish the installation with: python3 setup.py install\n\nFor usage instructions see: https://github.com/TinyMLaas/TinyML-CLI#usage\nMisc: if requirements.txt is overwritten by the generator use a different name, interface_requirements.txt etc (remember to install these in step 4). Using the CLI package might differ from using it locally, see Typer documentation: https://typer.tiangolo.com/tutorial/package/",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#bridge",
    "href": "docs/tinymlaas/Next_steps.html#bridge",
    "title": "Suggestions for further development",
    "section": "Bridge",
    "text": "Bridge\n\nSupport for Raspberry PI\nInstallation and observation for Arduino nano 33 BLE has been imported fully to this version of the version. However, the Raspberry PI support has not been tested at all and most likely will not work out of the box. Adding support/making sure installation to Raspberries work is a required feature, that unfortunately does not exist yet.\n\n\nBridge port\nA bridge can be saved to the backend as an IP address and as an URL. There is right now no validation to make sure that the given IP address or URL is a valid address. Also, if IP address is used, it automatically asumes that the bridge is hosted on port 5000. This should be changed so that the bridge can be hosted on any port.\n\n\nError handling\nThere is little to none error handling on the bridge. This means that, even if operations fail, it will still say the operation was successfull. The next ones are known errors that do not have error handling:\n\nWhen the compiled arduino sketch is uploaded successfully, but it can not be started for some reason, most likely because there isn’t enough memory for model on the device.\nObservation doesn’t have permission to the device.\n\nThe installation process has a chance to fail when running inside a docker container and the reason is unknown.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html",
    "title": "Arduino sketch",
    "section": "",
    "text": "This is a arduino sktech for image recognition with the help of machine learning. It has been designed to be used on the Arduino Nano 33 BLE with the OV7675 camera module.\nAt the moment, it can be used for binary detection, for example weather there is a person in the picture.\n\n\nThis sketch depends on Tensorflow lite for microcontrollers.\n\n\n\n\n\nFirst, install the required Arduino tflite library. On linux, this is typically in ~/Arduino/libraries, on MacOS in ~/Documents/Arduino/libraries and on Windows in My Documents\\Arduino\\Libraries.\nOnce in this directory, download the library with\ngit clone https://github.com/tensorflow/tflite-micro-arduino-examples Arduino_TensorFlowLite\nNow you can continue to installation. First, install the core library for the used arduino board.\narduino-cli core install arduino:mbed_nano\nNext, compile the sketch.\narduino-cli compile --fqbn arduino:mbed_nano:nano33ble template/\nFinally, install it to the device. For this, the port to which the device is connected to is required. This can be found with\narduino-cli board list\nCheck the name of the port, for example /dev/ttyACM0. Now, install the sketch with\narduino-cli upload -p &lt;device_port&gt; --fqbn arduino:mbed_nano:nano33ble template/\n\n\n\nThere is a provided Dockerfile which can make the installation a lot easier.\nFirst, build the image with docker. You can give it any other tag, but this example will name it arduino.\ndocker build -t arduino .\nThe image will contain the compiled sketch and will have arduino-cli as its entry point. Now, just install it with\ndocker run arduino upload -p &lt;device_port&gt; --fqbn arduino_mbed_nano:nano33ble template\nAgain, if you dont no what port the device is connected to, you can use the image to find that.\ndocker run arduino board list\n\n\n\n\nIn order to use your own tensorflow model, replace the target_model.cpp with your own model. This is a C array generated from the wanted model. To generate the C array from a tflite model, xxd can be used:\nxxd -i &lt;your_tflite_model&gt; &gt; target_model.cc\nThen, replace the the old file with this new generated file.\n\n\nIf you want to rename the model, you need to also change the name of the models header file target_model.h to the same name and change the headerfile name in template.ino.\n:warning: NOTE! Be careful when renaming the model. For some models the model can not be named something that starts with the word model. For this reason, it is adviced to always name the C-array file so that it does not start with model. ⚠️",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#dependencies",
    "title": "Arduino sketch",
    "section": "",
    "text": "This sketch depends on Tensorflow lite for microcontrollers.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#installing-sktech-to-microcontoller",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#installing-sktech-to-microcontoller",
    "title": "Arduino sketch",
    "section": "",
    "text": "First, install the required Arduino tflite library. On linux, this is typically in ~/Arduino/libraries, on MacOS in ~/Documents/Arduino/libraries and on Windows in My Documents\\Arduino\\Libraries.\nOnce in this directory, download the library with\ngit clone https://github.com/tensorflow/tflite-micro-arduino-examples Arduino_TensorFlowLite\nNow you can continue to installation. First, install the core library for the used arduino board.\narduino-cli core install arduino:mbed_nano\nNext, compile the sketch.\narduino-cli compile --fqbn arduino:mbed_nano:nano33ble template/\nFinally, install it to the device. For this, the port to which the device is connected to is required. This can be found with\narduino-cli board list\nCheck the name of the port, for example /dev/ttyACM0. Now, install the sketch with\narduino-cli upload -p &lt;device_port&gt; --fqbn arduino:mbed_nano:nano33ble template/\n\n\n\nThere is a provided Dockerfile which can make the installation a lot easier.\nFirst, build the image with docker. You can give it any other tag, but this example will name it arduino.\ndocker build -t arduino .\nThe image will contain the compiled sketch and will have arduino-cli as its entry point. Now, just install it with\ndocker run arduino upload -p &lt;device_port&gt; --fqbn arduino_mbed_nano:nano33ble template\nAgain, if you dont no what port the device is connected to, you can use the image to find that.\ndocker run arduino board list",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#changing-the-model",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#changing-the-model",
    "title": "Arduino sketch",
    "section": "",
    "text": "In order to use your own tensorflow model, replace the target_model.cpp with your own model. This is a C array generated from the wanted model. To generate the C array from a tflite model, xxd can be used:\nxxd -i &lt;your_tflite_model&gt; &gt; target_model.cc\nThen, replace the the old file with this new generated file.\n\n\nIf you want to rename the model, you need to also change the name of the models header file target_model.h to the same name and change the headerfile name in template.ino.\n:warning: NOTE! Be careful when renaming the model. For some models the model can not be named something that starts with the word model. For this reason, it is adviced to always name the C-array file so that it does not start with model. ⚠️",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html",
    "href": "docs/tinymlaas/Technologies.html",
    "title": "Technological choices",
    "section": "",
    "text": "This document is meant to answer why certain technological choices have been made and why certain frameworks have been chosen.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#main",
    "href": "docs/tinymlaas/Technologies.html#main",
    "title": "Technological choices",
    "section": "Main",
    "text": "Main\nThe TinyMLaaS-main -repository is meant for mostly building the whole software with docker and documentation. However, some development is also done in this repository. All tensorflow modules are coded here in Jupyter notebooks.\n\nJupyter notebooks and NBDEV\nAs jupyter notebooks can not be used as modules in python, they need to be exported into python modules. This is done with NBDEV. Nbdev also automatically creates documentation from the jupyter notebooks and deploys them to Github pages\n\n\nDocker\nRunning the software is meant to be done with docker. Docker allows running the software on different computers, without the software being platform spesific. Also, all the dependencies required for the software do not need to be installed on the host, rather, they will all be installed in the seperate docker container. If you are not familiar with docker, check out University of Helsinkis course Devops with Docker materials to get a basic understanding of docker.\n\nSysbox\nThere are parts of the software that require starting their own docker containers. For example, the relay will start containers to compile arduino sketches and to install them to the devices. When running the relay itself inside a docker container, there are a few ways of starting a new docker container from this docker container. First, is by using so called sibling containers. This gives the docker container access to the host machines docker socket, which allows it to control other containers on the host machine. However, this has a big security flaw, as if someone gets access to this docker container, they will be able to control all other docker containers on the host machine and start their own containers. The other way is by using docker inside docker, which allows docker containers to be started inside the docker container recursively. This approach requires privileged mode to be set for the docker container, meaning that it will have root privileges on the host machine. In order to run docker machines without privileged mode, Sysbox runtime is used. This allows starting docker containers inside the docker containers without having the docker container in privileged mode, which is a lot more secure and allows for better isolation of the docker containers.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#frontend",
    "href": "docs/tinymlaas/Technologies.html#frontend",
    "title": "Technological choices",
    "section": "Frontend",
    "text": "Frontend\n\nStreamlit\nThe frontend of the software is build with Streamlit. This is done to make the development process faster, as this frontend is mainly meant for demo purposes. Streamlit makes it easy to create good looking websites, however, there isn’t much room for cutomization and some features can be quite difficult to create.\nThere is still a dependecy on usbutils. This will be talked more about in the Bridge-section",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#backend",
    "href": "docs/tinymlaas/Technologies.html#backend",
    "title": "Technological choices",
    "section": "Backend",
    "text": "Backend\nThe backend is the heart of the software. It does all the communication between all the other modules and does a lot of the heavy lifting of the software. It is created as a API.\n\nFastAPI\nThe backend is created with FastAPI. FastAPI is very powerful for creating API:s, as it has great data validation with the help of Pydantic, it automatically creates good documentation about the different API requests and is simple to understand. To checkout more, read the Starting documentation.\nWhen deploying the API to production, the api will most likely be behind a proxy with some URL that has prefixes. For example, it might be deployed to example.uri.com/api/. For the API to function correctly, the root-path of the API needs to be declared for FastAPI, in this case, /api/.\n\n\nSQLAlchemy\nThe backend talks with the database with sqlalchemy. This means that it is able to talk with any SQL-database without any changes to the backends software.\nAs of now, the database in use is sqlite. However, this is meant to be more of a temporary solution to make development easier. For more information, checkout the suggestions in Suggestions for further development",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#bridge",
    "href": "docs/tinymlaas/Technologies.html#bridge",
    "title": "Technological choices",
    "section": "Bridge",
    "text": "Bridge\nThe relay is the part of the software to which the microcontrollers are connected to. It is also done in API style.\n\nFlask\nUnlike the backend, the bridge is created with Flask. Flask is lightweight and easy to understand, which makes sence for the bridge, as the hardware, on which the bridge runs, might not be that powerfull.\n\n\nUsbutils\nTo find USB-devices, the software does not use pythons libraries, such as PyUSB. This is because these softwares also have OS dependencies, that need to be installed and do not work that well in docker containers. USButils works great in contianers and is easy to install, which is why it has been chosen over pythons own libraries.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#command-line-interface",
    "href": "docs/tinymlaas/Technologies.html#command-line-interface",
    "title": "Technological choices",
    "section": "Command-line interface",
    "text": "Command-line interface\nAn API client is automatically generated from the OpenAPI definition provided by FastAPI. The generation is done with OpenAPI Generator. A command line tool interfacing the autogenerated client is built with Typer.\n\nOpen API Generator\nOpenAPI Generator enables building extensive Python clients with documentation. Generating a client makes it easy to design customized workflows around the API. This project uses the client as the main component of the command-line interface. The autogeneration also builds templates for tests. More generally using a generator was a way to test automatic code generation.\n\n\nTyper\nTyper is an easy to use Python library for building command-line interfaces. Typer can be used to build light weight CLI’s so it’s a good fit for the autogenerated client. Typer also uses Python type hints and provides automatic help functions that include required arguments and a command’s description from docstrings.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html",
    "href": "tutorials/runwalkthrough.html",
    "title": "Run Walkthrough",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html#summary",
    "href": "tutorials/runwalkthrough.html#summary",
    "title": "Run Walkthrough",
    "section": "Summary",
    "text": "Summary\nThe video tutorial is directly obtained from our internal review process of MVP6. It covers the process of creating and reviewing two default runs via project dashboard. Starting with user registration, the current MVP creates a default project and redirects the user to the project dashboard, in which page the main operation takes place. With the project dashboard, the user can specify a model and a dataset to run inference on a selected device. By default, two versions of this inference task are executed (forming two separate runs): a version that uses the original model for inference via Pytorch runtime, and a version that uses a quantized model (8-bit quantization) via tinyruntime (our custom runtime). After both runs are finished, the frontend provides visualizations to compare the results (accuracy), speed, and size of the model for each inference task.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html#features-breakdown",
    "href": "tutorials/runwalkthrough.html#features-breakdown",
    "title": "Run Walkthrough",
    "section": "Features Breakdown",
    "text": "Features Breakdown\n\nRegistration & Project Setup\nThe registration process in MVP6 is simplified, only requiring the user’s email address and a secure password. Email verification is yet to be implemented at this stage. After a successful registration, the user is redirected to the project dashboard with a default name and description, both of which can be modified later on. New projects can be created via sidebar button, which requires a unique name and possibly some description. Different projects can be selected also via the sidebar, which navigates to the project dashboard page of the corresponding selected project.\n\n\nModel & Dataset Selection\nWe are currently using models and datasets uploaded to Hugging Face Hub. The user can choose any publicly accessible models or datasets from Hugging Face. However, the currently recommended ones are our own uploaded models (ninjalabo/resnet18, ninjalabo/resnet34, and ninjalabo/resnet50) and dataset (ninjalabo/imagenette2-320) for consistency in directory structure and accepted file formats.\nThe user can also upload models and datasets to our own UI, which accepts a model file model.pkl and a dataset file in .zip or .tar.gz format. For convenience, the user can download our example model and dataset to easily test this feature without compatability issue.\n\n\nDevice Selection\nThe user can setup and select an edge device in which the inference task(s) shall be executed. The user can register a device with information including its architecture, connectivity details, memory and storage limits, and installation option. Currently, we are supporting devices with arm64 or amd64 architectures, in which the model will be installed in the form of a Docker image in our VM. The model will be optimized to the selected device, and its performance on it is measured and reported (accuracy, execution time, and model size).",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  }
]